<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.4.555">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Nipun Batra">
<meta name="dcterms.date" content="2022-01-29">

<title>Nipun Batra Blog - Understanding KL-Divergence</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<script src="../site_libs/quarto-html/quarto.js"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting-dark.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="../site_libs/bootstrap/bootstrap-dark.min.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="dark">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top quarto-banner">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="../index.html">
    <span class="navbar-title">Nipun Batra Blog</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="https://nipunbatra.github.io"> 
<span class="menu-text">Homepage</span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
          <div class="quarto-navbar-tools">
  <a href="" class="quarto-color-scheme-toggle quarto-navigation-tool  px-1" onclick="window.quartoToggleColorScheme(); return false;" title="Toggle dark mode"><i class="bi"></i></a>
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <h1 class="title">Understanding KL-Divergence</h1>
                                <div class="quarto-categories">
                <div class="quarto-category">ML</div>
              </div>
                  </div>
  </div>
    
  
  <div class="quarto-title-meta">

      <div>
      <div class="quarto-title-meta-heading">Author</div>
      <div class="quarto-title-meta-contents">
               <p>Nipun Batra </p>
            </div>
    </div>
      
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">January 29, 2022</p>
      </div>
    </div>
    
      
    </div>
    
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#goals" id="toc-goals" class="nav-link active" data-scroll-target="#goals">Goals:</a></li>
  <li><a href="#basic-imports" id="toc-basic-imports" class="nav-link" data-scroll-target="#basic-imports">Basic Imports</a></li>
  <li><a href="#creating-distributions" id="toc-creating-distributions" class="nav-link" data-scroll-target="#creating-distributions">Creating distributions</a></li>
  <li><a href="#optimizing-the-kl-divergence-between-q-and-p" id="toc-optimizing-the-kl-divergence-between-q-and-p" class="nav-link" data-scroll-target="#optimizing-the-kl-divergence-between-q-and-p">Optimizing the KL-divergence between q and p</a></li>
  <li><a href="#animation" id="toc-animation" class="nav-link" data-scroll-target="#animation">Animation!</a></li>
  <li><a href="#finding-the-kl-divergence-for-two-distributions-from-different-families" id="toc-finding-the-kl-divergence-for-two-distributions-from-different-families" class="nav-link" data-scroll-target="#finding-the-kl-divergence-for-two-distributions-from-different-families">Finding the KL divergence for two distributions from different families</a></li>
  <li><a href="#optimizing-the-kl-divergence-for-two-distributions-from-different-families" id="toc-optimizing-the-kl-divergence-for-two-distributions-from-different-families" class="nav-link" data-scroll-target="#optimizing-the-kl-divergence-for-two-distributions-from-different-families">Optimizing the KL divergence for two distributions from different families</a></li>
  <li><a href="#optimizing-the-kl-divergence-between-two-2d-distributions" id="toc-optimizing-the-kl-divergence-between-two-2d-distributions" class="nav-link" data-scroll-target="#optimizing-the-kl-divergence-between-two-2d-distributions">Optimizing the KL-divergence between two 2d distributions</a></li>
  <li><a href="#to-fix" id="toc-to-fix" class="nav-link" data-scroll-target="#to-fix">To-FIX</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content quarto-banner-title-block" id="quarto-document-content">





<section id="goals" class="level3">
<h3 class="anchored" data-anchor-id="goals">Goals:</h3>
<section id="g1-given-probability-distributions-p-and-q-find-the-divergence-measure-of-similarity-between-them" class="level4">
<h4 class="anchored" data-anchor-id="g1-given-probability-distributions-p-and-q-find-the-divergence-measure-of-similarity-between-them">G1: Given probability distributions <span class="math inline">\(p\)</span> and <span class="math inline">\(q\)</span>, find the divergence (measure of similarity) between them</h4>
<p>Let us first look at G1. Look at the illustration below. We have a normal distribution <span class="math inline">\(p\)</span> and two other normal distributions <span class="math inline">\(q_1\)</span> and <span class="math inline">\(q_2\)</span>. Which of <span class="math inline">\(q_1\)</span> and <span class="math inline">\(q_2\)</span>, would we consider closer to <span class="math inline">\(p\)</span>? <span class="math inline">\(q_2\)</span>, right?</p>
<p><img src="dkl.png" class="img-fluid"></p>
<p>To understand the notion of similarity, we use a metric called the KL-divergence given as <span class="math inline">\(D_{KL}(a || b)\)</span> where <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span> are the two distributions.</p>
<p>For G1, we can say <span class="math inline">\(q_2\)</span> is closer to <span class="math inline">\(p\)</span> compared to <span class="math inline">\(q_1\)</span> as:</p>
<p><span class="math inline">\(D_{KL}(q_2 || p) \lt D_{KL}(q_1 || p)\)</span></p>
<p>For the above example, we have the values as <span class="math inline">\(D_{KL}(q_2|| p) = 0.07\)</span> and <span class="math inline">\(D_{KL}(q_1|| p)= 0.35\)</span></p>
</section>
<section id="g2-assuming-p-to-be-fixed-can-we-find-optimum-parameters-of-q-to-make-it-as-close-as-possible-to-p" class="level4">
<h4 class="anchored" data-anchor-id="g2-assuming-p-to-be-fixed-can-we-find-optimum-parameters-of-q-to-make-it-as-close-as-possible-to-p">G2: assuming <span class="math inline">\(p\)</span> to be fixed, can we find optimum parameters of <span class="math inline">\(q\)</span> to make it as close as possible to <span class="math inline">\(p\)</span></h4>
<p>The following GIF shows the process of finding the optimum set of parameters for a normal distribution <span class="math inline">\(q\)</span> so that it becomes as close as possible to <span class="math inline">\(p\)</span>. This is equivalent of minimizing <span class="math inline">\(D_{KL}(q || p)\)</span></p>
<p><img src="kl_qp.gif" class="img-fluid"></p>
<p>The following GIF shows the above but for a two-dimensional distribution.</p>
<p><img src="kl_qp_2.gif" class="img-fluid"></p>
</section>
<section id="g3-finding-the-distance-between-two-distributions-of-different-families" class="level4">
<h4 class="anchored" data-anchor-id="g3-finding-the-distance-between-two-distributions-of-different-families">G3: finding the “distance” between two distributions of different families</h4>
<p>The below image shows the KL-divergence between distribution 1 (mixture of Gaussians) and distribution 2 (Gaussian)</p>
<p><img src="dkl-different.png" class="img-fluid"></p>
</section>
<section id="g4-optimizing-the-distance-between-two-distributions-of-different-families" class="level4">
<h4 class="anchored" data-anchor-id="g4-optimizing-the-distance-between-two-distributions-of-different-families">G4: optimizing the “distance” between two distributions of different families</h4>
<p>The below GIF shows the optimization of the KL-divergence between distribution 1 (mixture of Gaussians) and distribution 2 (Gaussian)</p>
<p><img src="kl_qp_mg.gif" class="img-fluid"></p>
</section>
<section id="g5-approximating-the-kl-divergence" class="level4">
<h4 class="anchored" data-anchor-id="g5-approximating-the-kl-divergence">G5: Approximating the KL-divergence</h4>
</section>
<section id="g6-implementing-variational-inference-for-linear-regression" class="level4">
<h4 class="anchored" data-anchor-id="g6-implementing-variational-inference-for-linear-regression">G6: Implementing variational inference for linear regression</h4>
</section>
</section>
<section id="basic-imports" class="level3">
<h3 class="anchored" data-anchor-id="basic-imports">Basic Imports</h3>
<div id="f41ca63d" class="cell" data-execution_count="1">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> tensorflow <span class="im">as</span> tf</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> seaborn <span class="im">as</span> sns</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> tensorflow_probability <span class="im">as</span> tfp</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a>tfd <span class="op">=</span> tfp.distributions</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a>tfl <span class="op">=</span> tfp.layers</span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a>tfb <span class="op">=</span> tfp.bijectors</span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> tensorflow.keras.models <span class="im">import</span> Sequential</span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> tensorflow.keras.layers <span class="im">import</span> Dense</span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> tensorflow.keras.optimizers <span class="im">import</span> RMSprop</span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> tensorflow.keras.callbacks <span class="im">import</span> Callback</span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a>sns.reset_defaults()</span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a>sns.set_context(context<span class="op">=</span><span class="st">"talk"</span>, font_scale<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a><span class="op">%</span>matplotlib inline</span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a><span class="op">%</span>config InlineBackend.figure_format<span class="op">=</span><span class="st">'retina'</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="creating-distributions" class="level3">
<h3 class="anchored" data-anchor-id="creating-distributions">Creating distributions</h3>
<section id="creating-psimmathcaln1.00-4.00" class="level4">
<h4 class="anchored" data-anchor-id="creating-psimmathcaln1.00-4.00">Creating <span class="math inline">\(p\sim\mathcal{N}(1.00, 4.00)\)</span></h4>
<div id="4f2a0d23" class="cell" data-execution_count="2">
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a>p <span class="op">=</span> tfd.Normal(<span class="dv">1</span>, <span class="dv">4</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>2022-02-04 14:55:14.596076: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.</code></pre>
</div>
</div>
<div id="81df527e" class="cell" data-execution_count="3">
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a>z_values <span class="op">=</span> tf.linspace(<span class="op">-</span><span class="dv">5</span>, <span class="dv">15</span>, <span class="dv">200</span>)</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a>z_values <span class="op">=</span> tf.cast(z_values, tf.float32)</span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a>prob_values_p <span class="op">=</span> p.prob(z_values)</span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a>plt.plot(z_values, prob_values_p, label<span class="op">=</span><span class="vs">r"$p\sim\mathcal</span><span class="sc">{N}</span><span class="vs">(1.00, 4.00)$"</span>)</span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a>sns.despine()</span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a>plt.legend()</span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">"x"</span>)</span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">"PDF"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="2022-01-29-kl-divergence_files/figure-html/cell-4-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
</section>
<section id="creating-qsimmathcalnloc-scale" class="level4">
<h4 class="anchored" data-anchor-id="creating-qsimmathcalnloc-scale">Creating <span class="math inline">\(q\sim\mathcal{N}(loc, scale)\)</span></h4>
<div id="f69f03bd" class="cell" data-execution_count="4">
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> create_q(loc, scale):</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> tfd.Normal(loc, scale)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="generating-a-few-qs-for-different-location-and-scale-value" class="level4">
<h4 class="anchored" data-anchor-id="generating-a-few-qs-for-different-location-and-scale-value">Generating a few qs for different location and scale value</h4>
<div id="a3c04051" class="cell" data-execution_count="5">
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a>q <span class="op">=</span> {}</span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a>q[(<span class="dv">0</span>, <span class="dv">1</span>)] <span class="op">=</span> create_q(<span class="fl">0.0</span>, <span class="fl">1.0</span>)</span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> loc <span class="kw">in</span> [<span class="dv">0</span>, <span class="dv">1</span>]:</span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> scale <span class="kw">in</span> [<span class="dv">1</span>, <span class="dv">2</span>]:</span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a>        q[(loc, scale)] <span class="op">=</span> create_q(<span class="bu">float</span>(loc), <span class="bu">float</span>(scale))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="26a80e7a" class="cell" data-execution_count="6">
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a>plt.plot(z_values, prob_values_p, label<span class="op">=</span><span class="vs">r"$p\sim\mathcal</span><span class="sc">{N}</span><span class="vs">(1.00, 4.00)$"</span>, lw<span class="op">=</span><span class="dv">3</span>)</span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a>plt.plot(</span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a>    z_values,</span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a>    create_q(<span class="fl">0.0</span>, <span class="fl">2.0</span>).prob(z_values),</span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a>    label<span class="op">=</span><span class="vs">r"$q_1\sim\mathcal</span><span class="sc">{N}</span><span class="vs">(0.00, 2.00)$"</span>,</span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a>    lw<span class="op">=</span><span class="dv">2</span>,</span>
<span id="cb7-7"><a href="#cb7-7" aria-hidden="true" tabindex="-1"></a>    linestyle<span class="op">=</span><span class="st">"--"</span>,</span>
<span id="cb7-8"><a href="#cb7-8" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb7-9"><a href="#cb7-9" aria-hidden="true" tabindex="-1"></a>plt.plot(</span>
<span id="cb7-10"><a href="#cb7-10" aria-hidden="true" tabindex="-1"></a>    z_values,</span>
<span id="cb7-11"><a href="#cb7-11" aria-hidden="true" tabindex="-1"></a>    create_q(<span class="fl">1.0</span>, <span class="fl">3.0</span>).prob(z_values),</span>
<span id="cb7-12"><a href="#cb7-12" aria-hidden="true" tabindex="-1"></a>    label<span class="op">=</span><span class="vs">r"$q_2\sim\mathcal</span><span class="sc">{N}</span><span class="vs">(1.00, 3.00)$"</span>,</span>
<span id="cb7-13"><a href="#cb7-13" aria-hidden="true" tabindex="-1"></a>    lw<span class="op">=</span><span class="dv">2</span>,</span>
<span id="cb7-14"><a href="#cb7-14" aria-hidden="true" tabindex="-1"></a>    linestyle<span class="op">=</span><span class="st">"-."</span>,</span>
<span id="cb7-15"><a href="#cb7-15" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb7-16"><a href="#cb7-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-17"><a href="#cb7-17" aria-hidden="true" tabindex="-1"></a>plt.legend(bbox_to_anchor<span class="op">=</span>(<span class="fl">1.04</span>, <span class="dv">1</span>), borderaxespad<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb7-18"><a href="#cb7-18" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">"x"</span>)</span>
<span id="cb7-19"><a href="#cb7-19" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">"PDF"</span>)</span>
<span id="cb7-20"><a href="#cb7-20" aria-hidden="true" tabindex="-1"></a>sns.despine()</span>
<span id="cb7-21"><a href="#cb7-21" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb7-22"><a href="#cb7-22" aria-hidden="true" tabindex="-1"></a>plt.savefig(</span>
<span id="cb7-23"><a href="#cb7-23" aria-hidden="true" tabindex="-1"></a>    <span class="st">"dkl.png"</span>,</span>
<span id="cb7-24"><a href="#cb7-24" aria-hidden="true" tabindex="-1"></a>    dpi<span class="op">=</span><span class="dv">150</span>,</span>
<span id="cb7-25"><a href="#cb7-25" aria-hidden="true" tabindex="-1"></a>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="2022-01-29-kl-divergence_files/figure-html/cell-7-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<div id="aaf50a35" class="cell" data-execution_count="7">
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="co">#### Computing KL-divergence</span></span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a>q_0_2_dkl <span class="op">=</span> tfd.kl_divergence(create_q(<span class="fl">0.0</span>, <span class="fl">2.0</span>), p)</span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a>q_1_3_dkl <span class="op">=</span> tfd.kl_divergence(create_q(<span class="fl">1.0</span>, <span class="fl">3.0</span>), p)</span>
<span id="cb8-5"><a href="#cb8-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-6"><a href="#cb8-6" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"D_KL (q(0, 2)||p) = </span><span class="sc">{</span>q_0_2_dkl<span class="sc">:0.2f}</span><span class="ss">"</span>)</span>
<span id="cb8-7"><a href="#cb8-7" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"D_KL (q(1, 3)||p) = </span><span class="sc">{</span>q_1_3_dkl<span class="sc">:0.2f}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>D_KL (q(0, 2)||p) = 0.35
D_KL (q(1, 3)||p) = 0.07</code></pre>
</div>
</div>
<p>As mentioned earlier, clearly, <span class="math inline">\(q_2\sim\mathcal{N}(1.00, 3.00)\)</span> seems closer to <span class="math inline">\(p\)</span></p>
</section>
</section>
<section id="optimizing-the-kl-divergence-between-q-and-p" class="level3">
<h3 class="anchored" data-anchor-id="optimizing-the-kl-divergence-between-q-and-p">Optimizing the KL-divergence between q and p</h3>
<p>We could create a grid of (loc, scale) pairs and find the best, as shown below.</p>
<div id="6b6aa6d4" class="cell" data-execution_count="8">
<div class="sourceCode cell-code" id="cb10"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a>plt.plot(z_values, prob_values_p, label<span class="op">=</span><span class="vs">r"$p\sim\mathcal</span><span class="sc">{N}</span><span class="vs">(1.00, 4.00)$"</span>, lw<span class="op">=</span><span class="dv">5</span>)</span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-4"><a href="#cb10-4" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> loc <span class="kw">in</span> [<span class="dv">0</span>, <span class="dv">1</span>]:</span>
<span id="cb10-5"><a href="#cb10-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> scale <span class="kw">in</span> [<span class="dv">1</span>, <span class="dv">2</span>]:</span>
<span id="cb10-6"><a href="#cb10-6" aria-hidden="true" tabindex="-1"></a>        q_d <span class="op">=</span> q[(loc, scale)]</span>
<span id="cb10-7"><a href="#cb10-7" aria-hidden="true" tabindex="-1"></a>        kl_d <span class="op">=</span> tfd.kl_divergence(q[(loc, scale)], p)</span>
<span id="cb10-8"><a href="#cb10-8" aria-hidden="true" tabindex="-1"></a>        plt.plot(</span>
<span id="cb10-9"><a href="#cb10-9" aria-hidden="true" tabindex="-1"></a>            z_values,</span>
<span id="cb10-10"><a href="#cb10-10" aria-hidden="true" tabindex="-1"></a>            q_d.prob(z_values),</span>
<span id="cb10-11"><a href="#cb10-11" aria-hidden="true" tabindex="-1"></a>            label<span class="op">=</span><span class="vs">rf"$q\sim\mathcal</span><span class="ch">{{</span><span class="vs">N</span><span class="ch">}}</span><span class="vs">(</span><span class="sc">{</span>loc<span class="sc">}</span><span class="vs">, </span><span class="sc">{</span>scale<span class="sc">}</span><span class="vs">)$"</span></span>
<span id="cb10-12"><a href="#cb10-12" aria-hidden="true" tabindex="-1"></a>            <span class="op">+</span> <span class="st">"</span><span class="ch">\n</span><span class="st">"</span></span>
<span id="cb10-13"><a href="#cb10-13" aria-hidden="true" tabindex="-1"></a>            <span class="op">+</span> <span class="vs">rf"$D_</span><span class="ch">{{</span><span class="vs">KL</span><span class="ch">}}</span><span class="vs">(q||p)$ = </span><span class="sc">{</span>kl_d<span class="sc">:0.2f}</span><span class="vs">"</span>,</span>
<span id="cb10-14"><a href="#cb10-14" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb10-15"><a href="#cb10-15" aria-hidden="true" tabindex="-1"></a>plt.legend(bbox_to_anchor<span class="op">=</span>(<span class="fl">1.04</span>, <span class="dv">1</span>), borderaxespad<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb10-16"><a href="#cb10-16" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">"x"</span>)</span>
<span id="cb10-17"><a href="#cb10-17" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">"PDF"</span>)</span>
<span id="cb10-18"><a href="#cb10-18" aria-hidden="true" tabindex="-1"></a>sns.despine()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="2022-01-29-kl-divergence_files/figure-html/cell-9-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>Or, we could use continuous optimization to find the best loc and scale parameters for q.</p>
<div id="41172b4e" class="cell" data-execution_count="9">
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a>to_train_q <span class="op">=</span> tfd.Normal(</span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a>    loc<span class="op">=</span>tf.Variable(<span class="op">-</span><span class="fl">1.0</span>, name<span class="op">=</span><span class="st">"loc"</span>),</span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a>    scale<span class="op">=</span>tfp.util.TransformedVariable(<span class="fl">1.0</span>, bijector<span class="op">=</span>tfb.Exp(), name<span class="op">=</span><span class="st">"scale"</span>),</span>
<span id="cb11-4"><a href="#cb11-4" aria-hidden="true" tabindex="-1"></a>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="ab598e6f" class="cell" data-execution_count="10">
<div class="sourceCode cell-code" id="cb12"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a>to_train_q.trainable_variables</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>2022-02-04 14:55:19.564807: W tensorflow/python/util/util.cc:368] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.</code></pre>
</div>
<div class="cell-output cell-output-display" data-execution_count="10">
<pre><code>(&lt;tf.Variable 'loc:0' shape=() dtype=float32, numpy=-1.0&gt;,
 &lt;tf.Variable 'scale:0' shape=() dtype=float32, numpy=0.0&gt;)</code></pre>
</div>
</div>
<div id="a7625739" class="cell" data-execution_count="11">
<div class="sourceCode cell-code" id="cb15"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a><span class="at">@tf.function</span></span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> loss_and_grads(q_dist):</span>
<span id="cb15-3"><a href="#cb15-3" aria-hidden="true" tabindex="-1"></a>    <span class="cf">with</span> tf.GradientTape() <span class="im">as</span> tape:</span>
<span id="cb15-4"><a href="#cb15-4" aria-hidden="true" tabindex="-1"></a>        loss <span class="op">=</span> tfd.kl_divergence(q_dist, p)</span>
<span id="cb15-5"><a href="#cb15-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> loss, tape.gradient(loss, q_dist.trainable_variables)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="adf0e66d" class="cell" data-execution_count="12">
<div class="sourceCode cell-code" id="cb16"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a>optimizer <span class="op">=</span> tf.keras.optimizers.Adam(learning_rate<span class="op">=</span><span class="fl">0.01</span>)</span>
<span id="cb16-2"><a href="#cb16-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-3"><a href="#cb16-3" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">500</span>):</span>
<span id="cb16-4"><a href="#cb16-4" aria-hidden="true" tabindex="-1"></a>    loss, grads <span class="op">=</span> loss_and_grads(to_train_q)</span>
<span id="cb16-5"><a href="#cb16-5" aria-hidden="true" tabindex="-1"></a>    optimizer.apply_gradients(<span class="bu">zip</span>(grads, to_train_q.trainable_variables))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="e7fce5ae" class="cell" data-execution_count="13">
<div class="sourceCode cell-code" id="cb17"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a>to_train_q.loc, to_train_q.scale</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="13">
<pre><code>(&lt;tf.Variable 'loc:0' shape=() dtype=float32, numpy=0.98873746&gt;,
 &lt;TransformedVariable: name=scale, dtype=float32, shape=[], fn="exp", numpy=3.9999995&gt;)</code></pre>
</div>
</div>
<p>After training, we are able to recover the scale and loc very close to that of <span class="math inline">\(p\)</span></p>
</section>
<section id="animation" class="level3">
<h3 class="anchored" data-anchor-id="animation">Animation!</h3>
<div id="e652ac5e" class="cell" data-execution_count="14">
<div class="sourceCode cell-code" id="cb19"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> matplotlib <span class="im">import</span> animation</span>
<span id="cb19-2"><a href="#cb19-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-3"><a href="#cb19-3" aria-hidden="true" tabindex="-1"></a>fig <span class="op">=</span> plt.figure(tight_layout<span class="op">=</span><span class="va">True</span>, figsize<span class="op">=</span>(<span class="dv">8</span>, <span class="dv">4</span>))</span>
<span id="cb19-4"><a href="#cb19-4" aria-hidden="true" tabindex="-1"></a>ax <span class="op">=</span> fig.gca()</span>
<span id="cb19-5"><a href="#cb19-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-6"><a href="#cb19-6" aria-hidden="true" tabindex="-1"></a>to_train_q <span class="op">=</span> tfd.Normal(</span>
<span id="cb19-7"><a href="#cb19-7" aria-hidden="true" tabindex="-1"></a>    loc<span class="op">=</span>tf.Variable(<span class="fl">5.0</span>, name<span class="op">=</span><span class="st">"loc"</span>),</span>
<span id="cb19-8"><a href="#cb19-8" aria-hidden="true" tabindex="-1"></a>    scale<span class="op">=</span>tfp.util.TransformedVariable(<span class="fl">0.1</span>, bijector<span class="op">=</span>tfb.Exp(), name<span class="op">=</span><span class="st">"scale"</span>),</span>
<span id="cb19-9"><a href="#cb19-9" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb19-10"><a href="#cb19-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-11"><a href="#cb19-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-12"><a href="#cb19-12" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> animate(i):</span>
<span id="cb19-13"><a href="#cb19-13" aria-hidden="true" tabindex="-1"></a>    ax.clear()</span>
<span id="cb19-14"><a href="#cb19-14" aria-hidden="true" tabindex="-1"></a>    ax.plot(z_values, prob_values_p, label<span class="op">=</span><span class="vs">r"$p\sim\mathcal</span><span class="sc">{N}</span><span class="vs">(1.00, 4.00)$"</span>, lw<span class="op">=</span><span class="dv">5</span>)</span>
<span id="cb19-15"><a href="#cb19-15" aria-hidden="true" tabindex="-1"></a>    loss, grads <span class="op">=</span> loss_and_grads(to_train_q)</span>
<span id="cb19-16"><a href="#cb19-16" aria-hidden="true" tabindex="-1"></a>    optimizer.apply_gradients(<span class="bu">zip</span>(grads, to_train_q.trainable_variables))</span>
<span id="cb19-17"><a href="#cb19-17" aria-hidden="true" tabindex="-1"></a>    loc <span class="op">=</span> to_train_q.loc.numpy()</span>
<span id="cb19-18"><a href="#cb19-18" aria-hidden="true" tabindex="-1"></a>    scale <span class="op">=</span> to_train_q.scale.numpy()</span>
<span id="cb19-19"><a href="#cb19-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-20"><a href="#cb19-20" aria-hidden="true" tabindex="-1"></a>    ax.plot(</span>
<span id="cb19-21"><a href="#cb19-21" aria-hidden="true" tabindex="-1"></a>        z_values,</span>
<span id="cb19-22"><a href="#cb19-22" aria-hidden="true" tabindex="-1"></a>        to_train_q.prob(z_values),</span>
<span id="cb19-23"><a href="#cb19-23" aria-hidden="true" tabindex="-1"></a>        label<span class="op">=</span><span class="vs">rf"$q\sim \mathcal</span><span class="ch">{{</span><span class="vs">N</span><span class="ch">}}</span><span class="vs">(</span><span class="sc">{</span>loc<span class="sc">:0.2f}</span><span class="vs">, </span><span class="sc">{</span>scale<span class="sc">:0.2f}</span><span class="vs">)$"</span>,</span>
<span id="cb19-24"><a href="#cb19-24" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb19-25"><a href="#cb19-25" aria-hidden="true" tabindex="-1"></a>    d_kl <span class="op">=</span> tfd.kl_divergence(to_train_q, p)</span>
<span id="cb19-26"><a href="#cb19-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-27"><a href="#cb19-27" aria-hidden="true" tabindex="-1"></a>    ax.set_title(<span class="vs">rf"Iteration: </span><span class="sc">{</span>i<span class="sc">}</span><span class="vs">, $D_</span><span class="ch">{{</span><span class="vs">KL</span><span class="ch">}}</span><span class="vs">(q||p)$: </span><span class="sc">{</span>d_kl<span class="sc">:0.2f}</span><span class="vs">"</span>)</span>
<span id="cb19-28"><a href="#cb19-28" aria-hidden="true" tabindex="-1"></a>    ax.legend(bbox_to_anchor<span class="op">=</span>(<span class="fl">1.1</span>, <span class="dv">1</span>), borderaxespad<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb19-29"><a href="#cb19-29" aria-hidden="true" tabindex="-1"></a>    ax.set_ylim((<span class="dv">0</span>, <span class="dv">1</span>))</span>
<span id="cb19-30"><a href="#cb19-30" aria-hidden="true" tabindex="-1"></a>    ax.set_xlim((<span class="op">-</span><span class="dv">5</span>, <span class="dv">15</span>))</span>
<span id="cb19-31"><a href="#cb19-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-32"><a href="#cb19-32" aria-hidden="true" tabindex="-1"></a>    ax.set_xlabel(<span class="st">"x"</span>)</span>
<span id="cb19-33"><a href="#cb19-33" aria-hidden="true" tabindex="-1"></a>    ax.set_ylabel(<span class="st">"PDF"</span>)</span>
<span id="cb19-34"><a href="#cb19-34" aria-hidden="true" tabindex="-1"></a>    sns.despine()</span>
<span id="cb19-35"><a href="#cb19-35" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-36"><a href="#cb19-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-37"><a href="#cb19-37" aria-hidden="true" tabindex="-1"></a>ani <span class="op">=</span> animation.FuncAnimation(fig, animate, frames<span class="op">=</span><span class="dv">150</span>)</span>
<span id="cb19-38"><a href="#cb19-38" aria-hidden="true" tabindex="-1"></a>plt.close()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="66485b26" class="cell" data-execution_count="15">
<div class="sourceCode cell-code" id="cb20"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a>ani.save(<span class="st">"kl_qp.gif"</span>, writer<span class="op">=</span><span class="st">"imagemagick"</span>, fps<span class="op">=</span><span class="dv">15</span>, dpi<span class="op">=</span><span class="dv">100</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<pre><code>&lt;Figure size 432x288 with 0 Axes&gt;</code></pre>
</div>
</div>
<p><img src="kl_qp.gif" class="img-fluid"></p>
</section>
<section id="finding-the-kl-divergence-for-two-distributions-from-different-families" class="level3">
<h3 class="anchored" data-anchor-id="finding-the-kl-divergence-for-two-distributions-from-different-families">Finding the KL divergence for two distributions from different families</h3>
<p>Let us rework our example with <code>p</code> coming from a mixture of Gaussian distribution and <code>q</code> being Normal.</p>
<div id="569b40e3" class="cell" data-execution_count="16">
<div class="sourceCode cell-code" id="cb22"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb22-1"><a href="#cb22-1" aria-hidden="true" tabindex="-1"></a>p_s <span class="op">=</span> tfd.MixtureSameFamily(</span>
<span id="cb22-2"><a href="#cb22-2" aria-hidden="true" tabindex="-1"></a>    mixture_distribution<span class="op">=</span>tfd.Categorical(probs<span class="op">=</span>[<span class="fl">0.5</span>, <span class="fl">0.5</span>]),</span>
<span id="cb22-3"><a href="#cb22-3" aria-hidden="true" tabindex="-1"></a>    components_distribution<span class="op">=</span>tfd.Normal(</span>
<span id="cb22-4"><a href="#cb22-4" aria-hidden="true" tabindex="-1"></a>        loc<span class="op">=</span>[<span class="op">-</span><span class="fl">0.2</span>, <span class="dv">1</span>], scale<span class="op">=</span>[<span class="fl">0.1</span>, <span class="fl">0.5</span>]  <span class="co"># One for each component.</span></span>
<span id="cb22-5"><a href="#cb22-5" aria-hidden="true" tabindex="-1"></a>    ),</span>
<span id="cb22-6"><a href="#cb22-6" aria-hidden="true" tabindex="-1"></a>)  <span class="co"># And same here.</span></span>
<span id="cb22-7"><a href="#cb22-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-8"><a href="#cb22-8" aria-hidden="true" tabindex="-1"></a>p_s</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="16">
<pre><code>&lt;tfp.distributions.MixtureSameFamily 'MixtureSameFamily' batch_shape=[] event_shape=[] dtype=float32&gt;</code></pre>
</div>
</div>
<div id="8ccbf347" class="cell" data-execution_count="17">
<div class="sourceCode cell-code" id="cb24"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb24-1"><a href="#cb24-1" aria-hidden="true" tabindex="-1"></a>plt.plot(z_values, p_s.prob(z_values))</span>
<span id="cb24-2"><a href="#cb24-2" aria-hidden="true" tabindex="-1"></a>sns.despine()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="2022-01-29-kl-divergence_files/figure-html/cell-18-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>Let us create two Normal distributions q_1 and q_2 and plot them to see which looks closer to p_s.</p>
<div id="fb0d7924" class="cell" data-execution_count="18">
<div class="sourceCode cell-code" id="cb25"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb25-1"><a href="#cb25-1" aria-hidden="true" tabindex="-1"></a>q_1 <span class="op">=</span> create_q(<span class="dv">3</span>, <span class="dv">1</span>)</span>
<span id="cb25-2"><a href="#cb25-2" aria-hidden="true" tabindex="-1"></a>q_2 <span class="op">=</span> create_q(<span class="dv">3</span>, <span class="fl">4.5</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="3c9ec63d" class="cell" data-execution_count="19">
<div class="sourceCode cell-code" id="cb26"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb26-1"><a href="#cb26-1" aria-hidden="true" tabindex="-1"></a>prob_values_p_s <span class="op">=</span> p_s.prob(z_values)</span>
<span id="cb26-2"><a href="#cb26-2" aria-hidden="true" tabindex="-1"></a>prob_values_q_1 <span class="op">=</span> q_1.prob(z_values)</span>
<span id="cb26-3"><a href="#cb26-3" aria-hidden="true" tabindex="-1"></a>prob_values_q_2 <span class="op">=</span> q_2.prob(z_values)</span>
<span id="cb26-4"><a href="#cb26-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-5"><a href="#cb26-5" aria-hidden="true" tabindex="-1"></a>plt.plot(z_values, prob_values_p_s, label<span class="op">=</span><span class="vs">r"MOG"</span>)</span>
<span id="cb26-6"><a href="#cb26-6" aria-hidden="true" tabindex="-1"></a>plt.plot(z_values, prob_values_q_1, label<span class="op">=</span><span class="vs">r"$q_1\sim\mathcal</span><span class="sc">{N}</span><span class="vs"> (3, 1.0)$"</span>)</span>
<span id="cb26-7"><a href="#cb26-7" aria-hidden="true" tabindex="-1"></a>plt.plot(z_values, prob_values_q_2, label<span class="op">=</span><span class="vs">r"$q_2\sim\mathcal</span><span class="sc">{N}</span><span class="vs"> (3, 4.5)$"</span>)</span>
<span id="cb26-8"><a href="#cb26-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-9"><a href="#cb26-9" aria-hidden="true" tabindex="-1"></a>sns.despine()</span>
<span id="cb26-10"><a href="#cb26-10" aria-hidden="true" tabindex="-1"></a>plt.legend()</span>
<span id="cb26-11"><a href="#cb26-11" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">"x"</span>)</span>
<span id="cb26-12"><a href="#cb26-12" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">"PDF"</span>)</span>
<span id="cb26-13"><a href="#cb26-13" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb26-14"><a href="#cb26-14" aria-hidden="true" tabindex="-1"></a>plt.savefig(</span>
<span id="cb26-15"><a href="#cb26-15" aria-hidden="true" tabindex="-1"></a>    <span class="st">"dkl-different.png"</span>,</span>
<span id="cb26-16"><a href="#cb26-16" aria-hidden="true" tabindex="-1"></a>    dpi<span class="op">=</span><span class="dv">150</span>,</span>
<span id="cb26-17"><a href="#cb26-17" aria-hidden="true" tabindex="-1"></a>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="2022-01-29-kl-divergence_files/figure-html/cell-20-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<div id="e0117731" class="cell" data-execution_count="20">
<div class="sourceCode cell-code" id="cb27"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb27-1"><a href="#cb27-1" aria-hidden="true" tabindex="-1"></a><span class="cf">try</span>:</span>
<span id="cb27-2"><a href="#cb27-2" aria-hidden="true" tabindex="-1"></a>    tfd.kl_divergence(q_1, p_s)</span>
<span id="cb27-3"><a href="#cb27-3" aria-hidden="true" tabindex="-1"></a><span class="cf">except</span> <span class="pp">Exception</span> <span class="im">as</span> e:</span>
<span id="cb27-4"><a href="#cb27-4" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(e)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>No KL(distribution_a || distribution_b) registered for distribution_a type Normal and distribution_b type MixtureSameFamily</code></pre>
</div>
</div>
<p>As we see above, we can not compute the KL divergence directly. The core idea would now be to leverage the Monte Carlo sampling and generating the expectation. The following function does that.</p>
<div id="043194fa" class="cell" data-execution_count="21">
<div class="sourceCode cell-code" id="cb29"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb29-1"><a href="#cb29-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> kl_via_sampling(q, p, n_samples<span class="op">=</span><span class="dv">100000</span>):</span>
<span id="cb29-2"><a href="#cb29-2" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Get samples from q</span></span>
<span id="cb29-3"><a href="#cb29-3" aria-hidden="true" tabindex="-1"></a>    sample_set <span class="op">=</span> q.sample(n_samples)</span>
<span id="cb29-4"><a href="#cb29-4" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Use the definition of KL-divergence</span></span>
<span id="cb29-5"><a href="#cb29-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> tf.reduce_mean(q.log_prob(sample_set) <span class="op">-</span> p.log_prob(sample_set))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="a6935fc4" class="cell" data-execution_count="22">
<div class="sourceCode cell-code" id="cb30"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb30-1"><a href="#cb30-1" aria-hidden="true" tabindex="-1"></a>kl_via_sampling(q_1, p_s), kl_via_sampling(q_2, p_s)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="22">
<pre><code>(&lt;tf.Tensor: shape=(), dtype=float32, numpy=9.465648&gt;,
 &lt;tf.Tensor: shape=(), dtype=float32, numpy=46.48004&gt;)</code></pre>
</div>
</div>
<p>As we can see from KL divergence calculations, <code>q_1</code> is closer to our Gaussian mixture distribution.</p>
</section>
<section id="optimizing-the-kl-divergence-for-two-distributions-from-different-families" class="level3">
<h3 class="anchored" data-anchor-id="optimizing-the-kl-divergence-for-two-distributions-from-different-families">Optimizing the KL divergence for two distributions from different families</h3>
<p>We saw that we can calculate the KL divergence between two different distribution families via sampling. But, as we did earlier, will we be able to optimize the parameters of our target surrogate distribution? The answer is No! As we have introduced sampling. However, there is still a way – by reparameterization!</p>
<p>Our surrogate q in this case is parameterized by <code>loc</code> and <code>scale</code>. The key idea here is to generate samples from a standard normal distribution (loc=0, scale=1) and then apply an affine transformation on the generated samples to get the samples generated from q. See my other post on sampling from normal distribution to understand this better.</p>
<p>The loss can now be thought of as a function of <code>loc</code> and <code>scale</code>.</p>
<div id="f515f655" class="cell" data-execution_count="23">
<div class="sourceCode cell-code" id="cb32"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb32-1"><a href="#cb32-1" aria-hidden="true" tabindex="-1"></a>n_samples <span class="op">=</span> <span class="dv">1000</span></span>
<span id="cb32-2"><a href="#cb32-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-3"><a href="#cb32-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-4"><a href="#cb32-4" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> loss(loc, scale):</span>
<span id="cb32-5"><a href="#cb32-5" aria-hidden="true" tabindex="-1"></a>    q <span class="op">=</span> tfd.Normal(loc<span class="op">=</span>loc, scale<span class="op">=</span>scale)</span>
<span id="cb32-6"><a href="#cb32-6" aria-hidden="true" tabindex="-1"></a>    q_1 <span class="op">=</span> tfd.Normal(loc<span class="op">=</span><span class="fl">0.0</span>, scale<span class="op">=</span><span class="fl">1.0</span>)</span>
<span id="cb32-7"><a href="#cb32-7" aria-hidden="true" tabindex="-1"></a>    sample_set <span class="op">=</span> q_1.sample(n_samples)</span>
<span id="cb32-8"><a href="#cb32-8" aria-hidden="true" tabindex="-1"></a>    sample_set <span class="op">=</span> loc <span class="op">+</span> scale <span class="op">*</span> sample_set</span>
<span id="cb32-9"><a href="#cb32-9" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> tf.reduce_mean(q.log_prob(sample_set) <span class="op">-</span> p_s.log_prob(sample_set))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Having defined the loss above, we can now optimize <code>loc</code> and <code>scale</code> to minimize the KL-divergence.</p>
<div id="7f032981" class="cell" data-execution_count="56">
<div class="sourceCode cell-code" id="cb33"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb33-1"><a href="#cb33-1" aria-hidden="true" tabindex="-1"></a>optimizer <span class="op">=</span> tf.optimizers.Adam(learning_rate<span class="op">=</span><span class="fl">0.05</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="f159ac84" class="cell" data-execution_count="64">
<div class="sourceCode cell-code" id="cb34"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb34-1"><a href="#cb34-1" aria-hidden="true" tabindex="-1"></a>n_iter <span class="op">=</span> <span class="dv">150</span></span>
<span id="cb34-2"><a href="#cb34-2" aria-hidden="true" tabindex="-1"></a>location_array <span class="op">=</span> np.empty(n_iter, dtype<span class="op">=</span>np.float32)</span>
<span id="cb34-3"><a href="#cb34-3" aria-hidden="true" tabindex="-1"></a>scale_array <span class="op">=</span> np.empty(n_iter, dtype<span class="op">=</span>np.float32)</span>
<span id="cb34-4"><a href="#cb34-4" aria-hidden="true" tabindex="-1"></a>loss_array <span class="op">=</span> np.empty(n_iter, dtype<span class="op">=</span>np.float32)</span>
<span id="cb34-5"><a href="#cb34-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-6"><a href="#cb34-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-7"><a href="#cb34-7" aria-hidden="true" tabindex="-1"></a>loc <span class="op">=</span> tf.Variable(<span class="fl">3.0</span>, dtype<span class="op">=</span>tf.float32)</span>
<span id="cb34-8"><a href="#cb34-8" aria-hidden="true" tabindex="-1"></a>scale <span class="op">=</span> tf.Variable(<span class="fl">4.0</span>, dtype<span class="op">=</span>tf.float32)</span>
<span id="cb34-9"><a href="#cb34-9" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(n_iter):</span>
<span id="cb34-10"><a href="#cb34-10" aria-hidden="true" tabindex="-1"></a>    <span class="cf">with</span> tf.GradientTape(persistent<span class="op">=</span><span class="va">True</span>) <span class="im">as</span> tape:</span>
<span id="cb34-11"><a href="#cb34-11" aria-hidden="true" tabindex="-1"></a>        tape.watch(loc)</span>
<span id="cb34-12"><a href="#cb34-12" aria-hidden="true" tabindex="-1"></a>        tape.watch(scale)</span>
<span id="cb34-13"><a href="#cb34-13" aria-hidden="true" tabindex="-1"></a>        lo <span class="op">=</span> loss(loc, scale)</span>
<span id="cb34-14"><a href="#cb34-14" aria-hidden="true" tabindex="-1"></a>    [dl_loc, dl_scale] <span class="op">=</span> tape.gradient(lo, [loc, scale])</span>
<span id="cb34-15"><a href="#cb34-15" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> i <span class="op">%</span> <span class="dv">50</span> <span class="op">==</span> <span class="dv">0</span>:</span>
<span id="cb34-16"><a href="#cb34-16" aria-hidden="true" tabindex="-1"></a>        tf.<span class="bu">print</span>(lo, loc, scale)</span>
<span id="cb34-17"><a href="#cb34-17" aria-hidden="true" tabindex="-1"></a>    location_array[i] <span class="op">=</span> loc.numpy()</span>
<span id="cb34-18"><a href="#cb34-18" aria-hidden="true" tabindex="-1"></a>    scale_array[i] <span class="op">=</span> scale.numpy()</span>
<span id="cb34-19"><a href="#cb34-19" aria-hidden="true" tabindex="-1"></a>    loss_array[i] <span class="op">=</span> lo.numpy()</span>
<span id="cb34-20"><a href="#cb34-20" aria-hidden="true" tabindex="-1"></a>    optimizer.apply_gradients(<span class="bu">zip</span>([dl_loc, dl_scale], [loc, scale]))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>38.7589951 3 4
0.4969607 0.736858189 0.680736303
0.528585315 0.774057031 0.617758751</code></pre>
</div>
</div>
<div id="bbc4958e" class="cell" data-execution_count="65">
<div class="sourceCode cell-code" id="cb36"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb36-1"><a href="#cb36-1" aria-hidden="true" tabindex="-1"></a>q_s <span class="op">=</span> tfd.Normal(loc<span class="op">=</span>loc, scale<span class="op">=</span>scale)</span>
<span id="cb36-2"><a href="#cb36-2" aria-hidden="true" tabindex="-1"></a>q_s</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="65">
<pre><code>&lt;tfp.distributions.Normal 'Normal' batch_shape=[] event_shape=[] dtype=float32&gt;</code></pre>
</div>
</div>
<div id="d3affcae" class="cell" data-execution_count="66">
<div class="sourceCode cell-code" id="cb38"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb38-1"><a href="#cb38-1" aria-hidden="true" tabindex="-1"></a>prob_values_p_s <span class="op">=</span> p_s.prob(z_values)</span>
<span id="cb38-2"><a href="#cb38-2" aria-hidden="true" tabindex="-1"></a>prob_values_q_s <span class="op">=</span> q_s.prob(z_values)</span>
<span id="cb38-3"><a href="#cb38-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-4"><a href="#cb38-4" aria-hidden="true" tabindex="-1"></a>plt.plot(z_values, prob_values_p_s, label<span class="op">=</span><span class="vs">r"p"</span>)</span>
<span id="cb38-5"><a href="#cb38-5" aria-hidden="true" tabindex="-1"></a>plt.plot(z_values, prob_values_q_s, label<span class="op">=</span><span class="vs">r"q"</span>)</span>
<span id="cb38-6"><a href="#cb38-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-7"><a href="#cb38-7" aria-hidden="true" tabindex="-1"></a>sns.despine()</span>
<span id="cb38-8"><a href="#cb38-8" aria-hidden="true" tabindex="-1"></a>plt.legend()</span>
<span id="cb38-9"><a href="#cb38-9" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">"x"</span>)</span>
<span id="cb38-10"><a href="#cb38-10" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">"PDF"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="2022-01-29-kl-divergence_files/figure-html/cell-28-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<div id="a0706e2d" class="cell" data-execution_count="70">
<div class="sourceCode cell-code" id="cb39"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb39-1"><a href="#cb39-1" aria-hidden="true" tabindex="-1"></a>prob_values_p_s <span class="op">=</span> p_s.prob(z_values)</span>
<span id="cb39-2"><a href="#cb39-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-3"><a href="#cb39-3" aria-hidden="true" tabindex="-1"></a>fig <span class="op">=</span> plt.figure(tight_layout<span class="op">=</span><span class="va">True</span>, figsize<span class="op">=</span>(<span class="dv">8</span>, <span class="dv">4</span>))</span>
<span id="cb39-4"><a href="#cb39-4" aria-hidden="true" tabindex="-1"></a>ax <span class="op">=</span> fig.gca()</span>
<span id="cb39-5"><a href="#cb39-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-6"><a href="#cb39-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-7"><a href="#cb39-7" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> a(iteration):</span>
<span id="cb39-8"><a href="#cb39-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-9"><a href="#cb39-9" aria-hidden="true" tabindex="-1"></a>    ax.clear()</span>
<span id="cb39-10"><a href="#cb39-10" aria-hidden="true" tabindex="-1"></a>    loc <span class="op">=</span> location_array[iteration]</span>
<span id="cb39-11"><a href="#cb39-11" aria-hidden="true" tabindex="-1"></a>    scale <span class="op">=</span> scale_array[iteration]</span>
<span id="cb39-12"><a href="#cb39-12" aria-hidden="true" tabindex="-1"></a>    q_s <span class="op">=</span> tfd.Normal(loc<span class="op">=</span>loc, scale<span class="op">=</span>scale)</span>
<span id="cb39-13"><a href="#cb39-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-14"><a href="#cb39-14" aria-hidden="true" tabindex="-1"></a>    prob_values_q_s <span class="op">=</span> q_s.prob(z_values)</span>
<span id="cb39-15"><a href="#cb39-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-16"><a href="#cb39-16" aria-hidden="true" tabindex="-1"></a>    ax.plot(z_values, prob_values_p_s, label<span class="op">=</span><span class="vs">r"p"</span>)</span>
<span id="cb39-17"><a href="#cb39-17" aria-hidden="true" tabindex="-1"></a>    ax.plot(z_values, prob_values_q_s, label<span class="op">=</span><span class="vs">r"q"</span>)</span>
<span id="cb39-18"><a href="#cb39-18" aria-hidden="true" tabindex="-1"></a>    ax.set_title(<span class="ss">f"Iteration </span><span class="sc">{</span>iteration<span class="sc">}</span><span class="ss">, Loss: </span><span class="sc">{</span>loss_array[iteration]<span class="sc">:0.2f}</span><span class="ss">"</span>)</span>
<span id="cb39-19"><a href="#cb39-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-20"><a href="#cb39-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-21"><a href="#cb39-21" aria-hidden="true" tabindex="-1"></a>ani_mg <span class="op">=</span> animation.FuncAnimation(fig, a, frames<span class="op">=</span>n_iter)</span>
<span id="cb39-22"><a href="#cb39-22" aria-hidden="true" tabindex="-1"></a>plt.close()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="8f4cf01b" class="cell" data-execution_count="71">
<div class="sourceCode cell-code" id="cb40"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb40-1"><a href="#cb40-1" aria-hidden="true" tabindex="-1"></a>plt.plot(location_array, label<span class="op">=</span><span class="st">"loc"</span>)</span>
<span id="cb40-2"><a href="#cb40-2" aria-hidden="true" tabindex="-1"></a>plt.plot(scale_array, label<span class="op">=</span><span class="st">"scale"</span>)</span>
<span id="cb40-3"><a href="#cb40-3" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">"Iterations"</span>)</span>
<span id="cb40-4"><a href="#cb40-4" aria-hidden="true" tabindex="-1"></a>sns.despine()</span>
<span id="cb40-5"><a href="#cb40-5" aria-hidden="true" tabindex="-1"></a>plt.legend()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="2022-01-29-kl-divergence_files/figure-html/cell-30-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<div id="e013616d" class="cell" data-execution_count="72">
<div class="sourceCode cell-code" id="cb41"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb41-1"><a href="#cb41-1" aria-hidden="true" tabindex="-1"></a>ani_mg.save(<span class="st">"kl_qp_mg.gif"</span>, writer<span class="op">=</span><span class="st">"imagemagick"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p><img src="kl_qp_mg.gif" class="img-fluid"></p>
</section>
<section id="optimizing-the-kl-divergence-between-two-2d-distributions" class="level3">
<h3 class="anchored" data-anchor-id="optimizing-the-kl-divergence-between-two-2d-distributions">Optimizing the KL-divergence between two 2d distributions</h3>
<p>Let us now repeat the same procedure but for two 2d Normal distributions.</p>
<div id="972e7d6b-ce74-434c-b64d-be34ac43f3bb" class="cell" data-execution_count="30">
<div class="sourceCode cell-code" id="cb42"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb42-1"><a href="#cb42-1" aria-hidden="true" tabindex="-1"></a>p_2d <span class="op">=</span> tfd.MultivariateNormalFullCovariance(</span>
<span id="cb42-2"><a href="#cb42-2" aria-hidden="true" tabindex="-1"></a>    loc<span class="op">=</span>[<span class="fl">0.0</span>, <span class="fl">0.0</span>], covariance_matrix<span class="op">=</span>[[<span class="fl">1.0</span>, <span class="fl">0.5</span>], [<span class="fl">0.5</span>, <span class="dv">2</span>]]</span>
<span id="cb42-3"><a href="#cb42-3" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb42-4"><a href="#cb42-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-5"><a href="#cb42-5" aria-hidden="true" tabindex="-1"></a>to_train_q_2d_2 <span class="op">=</span> tfd.MultivariateNormalDiag(</span>
<span id="cb42-6"><a href="#cb42-6" aria-hidden="true" tabindex="-1"></a>    loc<span class="op">=</span>tf.Variable([<span class="fl">2.0</span>, <span class="op">-</span><span class="fl">2.0</span>], name<span class="op">=</span><span class="st">"loc"</span>),</span>
<span id="cb42-7"><a href="#cb42-7" aria-hidden="true" tabindex="-1"></a>    scale_diag<span class="op">=</span>tfp.util.TransformedVariable(</span>
<span id="cb42-8"><a href="#cb42-8" aria-hidden="true" tabindex="-1"></a>        [<span class="fl">1.0</span>, <span class="fl">2.0</span>], bijector<span class="op">=</span>tfb.Exp(), name<span class="op">=</span><span class="st">"scale"</span></span>
<span id="cb42-9"><a href="#cb42-9" aria-hidden="true" tabindex="-1"></a>    ),</span>
<span id="cb42-10"><a href="#cb42-10" aria-hidden="true" tabindex="-1"></a>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>WARNING:tensorflow:From /Users/nipun/miniforge3/lib/python3.9/site-packages/tensorflow_probability/python/distributions/distribution.py:342: MultivariateNormalFullCovariance.__init__ (from tensorflow_probability.python.distributions.mvn_full_covariance) is deprecated and will be removed after 2019-12-01.
Instructions for updating:
`MultivariateNormalFullCovariance` is deprecated, use `MultivariateNormalTriL(loc=loc, scale_tril=tf.linalg.cholesky(covariance_matrix))` instead.</code></pre>
</div>
</div>
<div id="7413efa0-5141-47b9-a23d-4507c4df87ad" class="cell" data-execution_count="31">
<div class="sourceCode cell-code" id="cb44"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb44-1"><a href="#cb44-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> mpl_toolkits.mplot3d <span class="im">import</span> Axes3D</span>
<span id="cb44-2"><a href="#cb44-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> matplotlib <span class="im">import</span> cm</span>
<span id="cb44-3"><a href="#cb44-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-4"><a href="#cb44-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-5"><a href="#cb44-5" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> make_pdf_2d_gaussian(mu, sigma, ax, title):</span>
<span id="cb44-6"><a href="#cb44-6" aria-hidden="true" tabindex="-1"></a>    N <span class="op">=</span> <span class="dv">60</span></span>
<span id="cb44-7"><a href="#cb44-7" aria-hidden="true" tabindex="-1"></a>    X <span class="op">=</span> np.linspace(<span class="op">-</span><span class="dv">3</span>, <span class="dv">3</span>, N)</span>
<span id="cb44-8"><a href="#cb44-8" aria-hidden="true" tabindex="-1"></a>    Y <span class="op">=</span> np.linspace(<span class="op">-</span><span class="dv">3</span>, <span class="dv">4</span>, N)</span>
<span id="cb44-9"><a href="#cb44-9" aria-hidden="true" tabindex="-1"></a>    X, Y <span class="op">=</span> np.meshgrid(X, Y)</span>
<span id="cb44-10"><a href="#cb44-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-11"><a href="#cb44-11" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Pack X and Y into a single 3-dimensional array</span></span>
<span id="cb44-12"><a href="#cb44-12" aria-hidden="true" tabindex="-1"></a>    pos <span class="op">=</span> np.empty(X.shape <span class="op">+</span> (<span class="dv">2</span>,))</span>
<span id="cb44-13"><a href="#cb44-13" aria-hidden="true" tabindex="-1"></a>    pos[:, :, <span class="dv">0</span>] <span class="op">=</span> X</span>
<span id="cb44-14"><a href="#cb44-14" aria-hidden="true" tabindex="-1"></a>    pos[:, :, <span class="dv">1</span>] <span class="op">=</span> Y</span>
<span id="cb44-15"><a href="#cb44-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-16"><a href="#cb44-16" aria-hidden="true" tabindex="-1"></a>    F <span class="op">=</span> tfd.MultivariateNormalFullCovariance(loc<span class="op">=</span>mu, covariance_matrix<span class="op">=</span>sigma)</span>
<span id="cb44-17"><a href="#cb44-17" aria-hidden="true" tabindex="-1"></a>    Z <span class="op">=</span> F.prob(pos)</span>
<span id="cb44-18"><a href="#cb44-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-19"><a href="#cb44-19" aria-hidden="true" tabindex="-1"></a>    sns.despine()</span>
<span id="cb44-20"><a href="#cb44-20" aria-hidden="true" tabindex="-1"></a>    ax.set_xlabel(<span class="vs">r"$x_1$"</span>)</span>
<span id="cb44-21"><a href="#cb44-21" aria-hidden="true" tabindex="-1"></a>    ax.set_ylabel(<span class="vs">r"$x_2$"</span>)</span>
<span id="cb44-22"><a href="#cb44-22" aria-hidden="true" tabindex="-1"></a>    ax.set_aspect(<span class="st">"equal"</span>)</span>
<span id="cb44-23"><a href="#cb44-23" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> title:</span>
<span id="cb44-24"><a href="#cb44-24" aria-hidden="true" tabindex="-1"></a>        ax.set_title(<span class="ss">f"$\mu$ = </span><span class="sc">{</span>mu<span class="sc">}</span><span class="ch">\n</span><span class="ss"> $\Sigma$ = </span><span class="sc">{</span>np<span class="sc">.</span>array(sigma)<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb44-25"><a href="#cb44-25" aria-hidden="true" tabindex="-1"></a>        ax.contour(X, Y, Z, cmap<span class="op">=</span><span class="st">"viridis"</span>, alpha<span class="op">=</span><span class="dv">1</span>, zorder<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb44-26"><a href="#cb44-26" aria-hidden="true" tabindex="-1"></a>    <span class="cf">else</span>:</span>
<span id="cb44-27"><a href="#cb44-27" aria-hidden="true" tabindex="-1"></a>        ax.contourf(X, Y, Z, cmap<span class="op">=</span><span class="st">"plasma"</span>, alpha<span class="op">=</span><span class="fl">0.1</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="6038700d-5f1d-457b-ac28-173d71533dc9" class="cell" data-execution_count="32">
<div class="sourceCode cell-code" id="cb45"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb45-1"><a href="#cb45-1" aria-hidden="true" tabindex="-1"></a>fig, ax <span class="op">=</span> plt.subplots()</span>
<span id="cb45-2"><a href="#cb45-2" aria-hidden="true" tabindex="-1"></a>make_pdf_2d_gaussian([<span class="fl">0.0</span>, <span class="fl">1.0</span>], [[<span class="fl">1.0</span>, <span class="fl">0.5</span>], [<span class="fl">0.5</span>, <span class="dv">2</span>]], ax, <span class="va">False</span>)</span>
<span id="cb45-3"><a href="#cb45-3" aria-hidden="true" tabindex="-1"></a>make_pdf_2d_gaussian(</span>
<span id="cb45-4"><a href="#cb45-4" aria-hidden="true" tabindex="-1"></a>    to_train_q_2d_2.loc.numpy(), to_train_q_2d_2.covariance().numpy(), ax, <span class="va">True</span></span>
<span id="cb45-5"><a href="#cb45-5" aria-hidden="true" tabindex="-1"></a>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="2022-01-29-kl-divergence_files/figure-html/cell-34-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>As we can see above, the two distributions look very different. We can calculate the KL-divergence as before.</p>
<div id="58b46128" class="cell" data-execution_count="33">
<div class="sourceCode cell-code" id="cb46"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb46-1"><a href="#cb46-1" aria-hidden="true" tabindex="-1"></a>tfd.kl_divergence(to_train_q_2d_2, p_2d)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="33">
<pre><code>&lt;tf.Tensor: shape=(), dtype=float32, numpy=4.8723755&gt;</code></pre>
</div>
</div>
<div id="ed6014ee-d894-4f49-abd3-9a3dac160a87" class="cell" data-execution_count="34">
<div class="sourceCode cell-code" id="cb48"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb48-1"><a href="#cb48-1" aria-hidden="true" tabindex="-1"></a>fig <span class="op">=</span> plt.figure(tight_layout<span class="op">=</span><span class="va">True</span>, figsize<span class="op">=</span>(<span class="dv">8</span>, <span class="dv">4</span>))</span>
<span id="cb48-2"><a href="#cb48-2" aria-hidden="true" tabindex="-1"></a>ax <span class="op">=</span> fig.gca()</span>
<span id="cb48-3"><a href="#cb48-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb48-4"><a href="#cb48-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb48-5"><a href="#cb48-5" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> animate(i):</span>
<span id="cb48-6"><a href="#cb48-6" aria-hidden="true" tabindex="-1"></a>    ax.clear()</span>
<span id="cb48-7"><a href="#cb48-7" aria-hidden="true" tabindex="-1"></a>    <span class="cf">with</span> tf.GradientTape() <span class="im">as</span> tape:</span>
<span id="cb48-8"><a href="#cb48-8" aria-hidden="true" tabindex="-1"></a>        loss <span class="op">=</span> tfd.kl_divergence(to_train_q_2d_2, p_2d)</span>
<span id="cb48-9"><a href="#cb48-9" aria-hidden="true" tabindex="-1"></a>        grads <span class="op">=</span> tape.gradient(loss, to_train_q_2d_2.trainable_variables)</span>
<span id="cb48-10"><a href="#cb48-10" aria-hidden="true" tabindex="-1"></a>        optimizer.apply_gradients(<span class="bu">zip</span>(grads, to_train_q_2d_2.trainable_variables))</span>
<span id="cb48-11"><a href="#cb48-11" aria-hidden="true" tabindex="-1"></a>    loc <span class="op">=</span> np.<span class="bu">round</span>(to_train_q_2d_2.loc.numpy(), <span class="dv">1</span>)</span>
<span id="cb48-12"><a href="#cb48-12" aria-hidden="true" tabindex="-1"></a>    scale <span class="op">=</span> np.<span class="bu">round</span>(to_train_q_2d_2.covariance().numpy(), <span class="dv">1</span>)</span>
<span id="cb48-13"><a href="#cb48-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb48-14"><a href="#cb48-14" aria-hidden="true" tabindex="-1"></a>    make_pdf_2d_gaussian(loc, scale, ax, <span class="va">True</span>)</span>
<span id="cb48-15"><a href="#cb48-15" aria-hidden="true" tabindex="-1"></a>    make_pdf_2d_gaussian([<span class="fl">0.0</span>, <span class="fl">1.0</span>], [[<span class="fl">1.0</span>, <span class="fl">0.5</span>], [<span class="fl">0.5</span>, <span class="dv">2</span>]], ax, <span class="va">False</span>)</span>
<span id="cb48-16"><a href="#cb48-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb48-17"><a href="#cb48-17" aria-hidden="true" tabindex="-1"></a>    ax.set_xlabel(<span class="vs">r"$x_1$"</span>)</span>
<span id="cb48-18"><a href="#cb48-18" aria-hidden="true" tabindex="-1"></a>    ax.set_ylabel(<span class="vs">r"$x_2$"</span>)</span>
<span id="cb48-19"><a href="#cb48-19" aria-hidden="true" tabindex="-1"></a>    ax.spines[<span class="st">"right"</span>].set_visible(<span class="va">False</span>)</span>
<span id="cb48-20"><a href="#cb48-20" aria-hidden="true" tabindex="-1"></a>    ax.spines[<span class="st">"top"</span>].set_visible(<span class="va">False</span>)</span>
<span id="cb48-21"><a href="#cb48-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb48-22"><a href="#cb48-22" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Only show ticks on the left and bottom spines</span></span>
<span id="cb48-23"><a href="#cb48-23" aria-hidden="true" tabindex="-1"></a>    ax.yaxis.set_ticks_position(<span class="st">"left"</span>)</span>
<span id="cb48-24"><a href="#cb48-24" aria-hidden="true" tabindex="-1"></a>    ax.xaxis.set_ticks_position(<span class="st">"bottom"</span>)</span>
<span id="cb48-25"><a href="#cb48-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb48-26"><a href="#cb48-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb48-27"><a href="#cb48-27" aria-hidden="true" tabindex="-1"></a>ani2 <span class="op">=</span> animation.FuncAnimation(fig, animate, frames<span class="op">=</span><span class="dv">100</span>)</span>
<span id="cb48-28"><a href="#cb48-28" aria-hidden="true" tabindex="-1"></a>plt.close()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="c3fed04e-a2d4-4491-868c-7353c8e0d7e3" class="cell" data-execution_count="35">
<div class="sourceCode cell-code" id="cb49"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb49-1"><a href="#cb49-1" aria-hidden="true" tabindex="-1"></a>ani2.save(<span class="st">"kl_qp_2.gif"</span>, writer<span class="op">=</span><span class="st">"imagemagick"</span>, fps<span class="op">=</span><span class="dv">15</span>, dpi<span class="op">=</span><span class="dv">100</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<pre><code>&lt;Figure size 432x288 with 0 Axes&gt;</code></pre>
</div>
</div>
<p><img src="kl_qp_2.gif" class="img-fluid"></p>
<div id="7bd62e09-898a-4084-880d-e71b9f84f5f1" class="cell" data-execution_count="36">
<div class="sourceCode cell-code" id="cb51"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb51-1"><a href="#cb51-1" aria-hidden="true" tabindex="-1"></a>to_train_q_2d_2.loc, to_train_q_2d_2.covariance()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="36">
<pre><code>(&lt;tf.Variable 'loc:0' shape=(2,) dtype=float32, numpy=array([ 0.01590762, -0.01590773], dtype=float32)&gt;,
 &lt;tf.Tensor: shape=(2, 2), dtype=float32, numpy=
 array([[0.87550056, 0.        ],
        [0.        , 1.7570419 ]], dtype=float32)&gt;)</code></pre>
</div>
</div>
<div id="e64516b3" class="cell" data-execution_count="37">
<div class="sourceCode cell-code" id="cb53"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb53-1"><a href="#cb53-1" aria-hidden="true" tabindex="-1"></a>tfd.kl_divergence(to_train_q_2d_2, p_2d)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="37">
<pre><code>&lt;tf.Tensor: shape=(), dtype=float32, numpy=0.0670591&gt;</code></pre>
</div>
</div>
<p>We can now see that the KL-divergence has reduced significantly from where we started, but it will unlikely improve as ou r <code>q</code> distribution is a multivariate diagonal normal distribution, meaning there is no correlation between the two dimensions.</p>
</section>
<section id="to-fix" class="level3">
<h3 class="anchored" data-anchor-id="to-fix">To-FIX</h3>
<p>Everything below here needs to be fixed</p>
<section id="kl-divergence-and-elbo" class="level4">
<h4 class="anchored" data-anchor-id="kl-divergence-and-elbo">KL-Divergence and ELBO</h4>
<p>Let us consider linear regression. We have parameters <span class="math inline">\(\theta \in R^D\)</span> and we define a prior over them. Let us assume we define prior <span class="math inline">\(p(\theta)\sim \mathcal{N_D} (\mu, \Sigma)\)</span>. Now, given our dataset <span class="math inline">\(D = \{X, y\}\)</span> and a parameter vector <span class="math inline">\(\theta\)</span>, we can deifine our likelihood as <span class="math inline">\(p(D|\theta)\)</span> or $p(y|X, ) = <em>{i=1}^{n} p(y_i|x_i, ) = </em>{i=1}^{n} (y_i|x_i^T, ^2) $</p>
<p>As per Bayes rule, we can obtain the posterior over <span class="math inline">\(\theta\)</span> as:</p>
<p><span class="math inline">\(p(\theta|D) = \dfrac{p(D|\theta)p(\theta)}{p(D)}\)</span></p>
<p>Now, in general <span class="math inline">\(p(D)\)</span> is hard to compute.</p>
<p>So, in variational inference, our aim is to use a surrogate distribution <span class="math inline">\(q(\theta)\)</span> such that it is very close to <span class="math inline">\(p(\theta|D)\)</span>. We do so by minimizing the KL divergence between <span class="math inline">\(q(\theta)\)</span> and <span class="math inline">\(p(\theta|D)\)</span>.</p>
<p>Aim: <span class="math display">\[q^*(\theta) = \underset{q(\theta) \in \mathcal{Q}}{\mathrm{argmin~}} D_{KL}[q(\theta)||p(\theta|D)]\]</span></p>
<p>Now, <span class="math display">\[D_{KL}[q(\theta)||p(\theta|D)] = \mathbb{E}_{q(\theta)}[\log\frac{q(\theta)}{p(\theta|D)}]\]</span> Now, <span class="math display">\[ = \mathbb{E}_{q(\theta)}[\log\frac{q(\theta)p(D)}{p(\theta, D)}]\]</span> Now, <span class="math display">\[ = \mathbb{E}_{q(\theta)}[\log q(\theta)]- \mathbb{E}_{q(\theta)}[\log p(\theta, D)] + \mathbb{E}_{q(\theta)}[\log p(D)] \]</span> <span class="math display">\[= \mathbb{E}_{q(\theta)}[\log q(\theta)]- \mathbb{E}_{q(\theta)}[\log p(\theta, D)] + \log p(D) \]</span></p>
<p>Now, <span class="math inline">\(p(D) \in \{0, 1\}\)</span>. Thus, <span class="math inline">\(\log p(D) \in \{-\infty, 0 \}\)</span></p>
<p>Now, let us look at the quantities:</p>
<p><span class="math display">\[\underbrace{D_{KL}[q(\theta)||p(\theta|D)]}_{\geq 0} = \underbrace{\mathbb{E}_{q(\theta)}[\log q(\theta)]- \mathbb{E}_{q(\theta)}[\log p(\theta, D)]}_{-\text{ELBO(q)}} +  \underbrace{\log p(D)}_{\leq 0}\]</span></p>
<p>Thus, we know that <span class="math inline">\(\log p(D) \geq \text{ELBO(q)}\)</span></p>
<p>Thus, finally we can rewrite the optimisation from</p>
<p><span class="math display">\[q^*(\theta) = \underset{q(\theta) \in \mathcal{Q}}{\mathrm{argmin~}} D_{KL}[q(\theta)||p(\theta|D)]\]</span></p>
<p>to</p>
<p><span class="math display">\[q^*(\theta) = \underset{q(\theta) \in \mathcal{Q}}{\mathrm{argmax~}} \text{ELBO(q)}\]</span></p>
<p>Now, given our linear regression problem setup, we want to maximize the ELBO.</p>
<p>We can do so by the following. As a simple example, let us assume <span class="math inline">\(\theta \in R^2\)</span></p>
<ul>
<li>Assume some q. Say, a Normal distribution. So, <span class="math inline">\(q\sim \mathcal{N}_2\)</span></li>
<li>Draw samples from q. Say N samples.</li>
<li>Initilize ELBO = 0.0</li>
<li>For each sample:
<ul>
<li>Let us assume drawn sample is <span class="math inline">\([\theta_1, \theta_2]^T\)</span></li>
<li>Compute log_prob of prior on <span class="math inline">\([\theta_1, \theta_2]^T\)</span> or <code>lp = p.log_prob(θ1, θ2)</code></li>
<li>Compute log_prob of likelihood on <span class="math inline">\([\theta_1, \theta_2]^T\)</span> or <code>ll = l.log_prob(θ1, θ2)</code></li>
<li>Compute log_prob of q on <span class="math inline">\([\theta_1, \theta_2]^T\)</span> or <code>lq = q.log_prob(θ1, θ2)</code></li>
<li>ELBO = ELBO + (ll+lp-q)</li>
</ul></li>
<li>Return ELBO/N</li>
</ul>
<div id="9a7f01ec" class="cell" data-execution_count="165">
<div class="sourceCode cell-code" id="cb55"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb55-1"><a href="#cb55-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> lr(x, stddv_datapoints):</span>
<span id="cb55-2"><a href="#cb55-2" aria-hidden="true" tabindex="-1"></a>    num_datapoints, data_dim <span class="op">=</span> x.shape</span>
<span id="cb55-3"><a href="#cb55-3" aria-hidden="true" tabindex="-1"></a>    th <span class="op">=</span> <span class="cf">yield</span> tfd.Normal(</span>
<span id="cb55-4"><a href="#cb55-4" aria-hidden="true" tabindex="-1"></a>        loc<span class="op">=</span>tf.zeros([data_dim <span class="op">+</span> <span class="dv">1</span>]), scale<span class="op">=</span>tf.ones([data_dim <span class="op">+</span> <span class="dv">1</span>]), name<span class="op">=</span><span class="st">"theta"</span></span>
<span id="cb55-5"><a href="#cb55-5" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb55-6"><a href="#cb55-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb55-7"><a href="#cb55-7" aria-hidden="true" tabindex="-1"></a>    x_dash <span class="op">=</span> tf.concat([tf.ones_like(x), x], <span class="dv">1</span>)</span>
<span id="cb55-8"><a href="#cb55-8" aria-hidden="true" tabindex="-1"></a>    y <span class="op">=</span> <span class="cf">yield</span> tfd.Normal(</span>
<span id="cb55-9"><a href="#cb55-9" aria-hidden="true" tabindex="-1"></a>        loc<span class="op">=</span>tf.linalg.matvec(x_dash, th), scale<span class="op">=</span>stddv_datapoints, name<span class="op">=</span><span class="st">"y"</span></span>
<span id="cb55-10"><a href="#cb55-10" aria-hidden="true" tabindex="-1"></a>    )</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="09a1088c" class="cell" data-execution_count="193">
<div class="sourceCode cell-code" id="cb56"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb56-1"><a href="#cb56-1" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> tf.linspace(<span class="op">-</span><span class="fl">5.0</span>, <span class="fl">5.0</span>, <span class="dv">100</span>)</span>
<span id="cb56-2"><a href="#cb56-2" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> tf.expand_dims(x, <span class="dv">1</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="5189b8df" class="cell" data-execution_count="237">
<div class="sourceCode cell-code" id="cb57"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb57-1"><a href="#cb57-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> functools</span>
<span id="cb57-2"><a href="#cb57-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-3"><a href="#cb57-3" aria-hidden="true" tabindex="-1"></a>stddv_datapoints <span class="op">=</span> <span class="dv">1</span></span>
<span id="cb57-4"><a href="#cb57-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-5"><a href="#cb57-5" aria-hidden="true" tabindex="-1"></a>concrete_lr_model <span class="op">=</span> functools.partial(lr_2, x<span class="op">=</span>x, stddv_datapoints<span class="op">=</span>stddv_datapoints)</span>
<span id="cb57-6"><a href="#cb57-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-7"><a href="#cb57-7" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> tfd.JointDistributionCoroutineAutoBatched(concrete_lr_model)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="2df8885c" class="cell" data-execution_count="238">
<div class="sourceCode cell-code" id="cb58"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb58-1"><a href="#cb58-1" aria-hidden="true" tabindex="-1"></a>tf.random.set_seed(<span class="dv">0</span>)</span>
<span id="cb58-2"><a href="#cb58-2" aria-hidden="true" tabindex="-1"></a>th_sample, data_sample <span class="op">=</span> model.sample()</span>
<span id="cb58-3"><a href="#cb58-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb58-4"><a href="#cb58-4" aria-hidden="true" tabindex="-1"></a>plt.scatter(x[:, <span class="dv">0</span>], data_sample, s<span class="op">=</span><span class="dv">10</span>)</span>
<span id="cb58-5"><a href="#cb58-5" aria-hidden="true" tabindex="-1"></a>plt.plot(x[:, <span class="dv">0</span>], tf.reshape(x<span class="op">*</span>th_sample[<span class="dv">1</span>] <span class="op">+</span> th_sample[<span class="dv">0</span>], [<span class="op">-</span><span class="dv">1</span>]))</span>
<span id="cb58-6"><a href="#cb58-6" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(th_sample)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>tf.Tensor([1.5110626  0.42292204], shape=(2,), dtype=float32)</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="2022-01-29-kl-divergence_files/figure-html/cell-43-output-2.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<div id="2ac3620a" class="cell" data-execution_count="239">
<div class="sourceCode cell-code" id="cb60"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb60-1"><a href="#cb60-1" aria-hidden="true" tabindex="-1"></a>model.log_prob(th_sample, data_sample)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="239">
<pre><code>&lt;tf.Tensor: shape=(), dtype=float32, numpy=-150.26591&gt;</code></pre>
</div>
</div>
<div id="0320e377" class="cell" data-execution_count="258">
<div class="sourceCode cell-code" id="cb62"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb62-1"><a href="#cb62-1" aria-hidden="true" tabindex="-1"></a>loc <span class="op">=</span> tf.Variable([<span class="fl">1.0</span>, <span class="fl">1.0</span>], dtype<span class="op">=</span>tf.float32)</span>
<span id="cb62-2"><a href="#cb62-2" aria-hidden="true" tabindex="-1"></a>scale <span class="op">=</span> tf.Variable([<span class="fl">1.0</span>, <span class="fl">1.0</span>])</span>
<span id="cb62-3"><a href="#cb62-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb62-4"><a href="#cb62-4" aria-hidden="true" tabindex="-1"></a>q_to_learn <span class="op">=</span> tfd.Normal(loc<span class="op">=</span>loc, scale<span class="op">=</span>scale, name<span class="op">=</span><span class="st">"q_theta_learn"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="859beaa0" class="cell" data-execution_count="345">
<div class="sourceCode cell-code" id="cb63"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb63-1"><a href="#cb63-1" aria-hidden="true" tabindex="-1"></a>n_samples <span class="op">=</span> <span class="dv">1000</span></span>
<span id="cb63-2"><a href="#cb63-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb63-3"><a href="#cb63-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb63-4"><a href="#cb63-4" aria-hidden="true" tabindex="-1"></a>loc <span class="op">=</span> tf.Variable([<span class="fl">1.0</span>, <span class="fl">0.4</span>], dtype<span class="op">=</span>tf.float32)</span>
<span id="cb63-5"><a href="#cb63-5" aria-hidden="true" tabindex="-1"></a>scale <span class="op">=</span> tfp.util.TransformedVariable([<span class="fl">.7</span>, <span class="fl">.6</span>], bijector<span class="op">=</span>tfb.SoftClip(<span class="fl">0.5</span>, <span class="fl">1.0</span>))</span>
<span id="cb63-6"><a href="#cb63-6" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> loss():</span>
<span id="cb63-7"><a href="#cb63-7" aria-hidden="true" tabindex="-1"></a>    q_to_learn <span class="op">=</span> tfd.Normal(loc<span class="op">=</span>loc, scale<span class="op">=</span>scale, name<span class="op">=</span><span class="st">"q_theta_learn"</span>)</span>
<span id="cb63-8"><a href="#cb63-8" aria-hidden="true" tabindex="-1"></a>    q_1 <span class="op">=</span> tfd.Normal(loc<span class="op">=</span>[<span class="fl">0.0</span>,<span class="fl">0.0</span>], scale<span class="op">=</span>[<span class="fl">1.0</span>, <span class="fl">1.0</span>])</span>
<span id="cb63-9"><a href="#cb63-9" aria-hidden="true" tabindex="-1"></a>    sample_set <span class="op">=</span> q_1.sample(n_samples)</span>
<span id="cb63-10"><a href="#cb63-10" aria-hidden="true" tabindex="-1"></a>    log_joint <span class="op">=</span> tf.reduce_sum(model.log_prob(sample_set, data_sample))</span>
<span id="cb63-11"><a href="#cb63-11" aria-hidden="true" tabindex="-1"></a>    log_q <span class="op">=</span> tf.reduce_sum(q_to_learn.log_prob(sample_set))</span>
<span id="cb63-12"><a href="#cb63-12" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> log_q <span class="op">-</span> log_joint</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="184d99f2" class="cell" data-execution_count="352">
<div class="sourceCode cell-code" id="cb64"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb64-1"><a href="#cb64-1" aria-hidden="true" tabindex="-1"></a>trace6000 <span class="op">=</span> tfp.math.minimize(loss_fn<span class="op">=</span>loss, num_steps<span class="op">=</span><span class="dv">6000</span>, </span>
<span id="cb64-2"><a href="#cb64-2" aria-hidden="true" tabindex="-1"></a>                  optimizer<span class="op">=</span>tf.optimizers.Adam(<span class="fl">0.0001</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="03fadfc2" class="cell" data-execution_count="347">
<div class="sourceCode cell-code" id="cb65"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb65-1"><a href="#cb65-1" aria-hidden="true" tabindex="-1"></a>loc</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="347">
<pre><code>&lt;tf.Variable 'Variable:0' shape=(2,) dtype=float32, numpy=array([1.4298061 , 0.84917635], dtype=float32)&gt;</code></pre>
</div>
</div>
<div id="4638ae79" class="cell" data-execution_count="348">
<div class="sourceCode cell-code" id="cb67"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb67-1"><a href="#cb67-1" aria-hidden="true" tabindex="-1"></a>scale</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="348">
<pre><code>&lt;TransformedVariable: name=soft_clip, dtype=float32, shape=[2], fn="soft_clip", numpy=array([0.6512191, 0.5705266], dtype=float32)&gt;</code></pre>
</div>
</div>
<div id="f01cef69" class="cell" data-execution_count="349">
<div class="sourceCode cell-code" id="cb69"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb69-1"><a href="#cb69-1" aria-hidden="true" tabindex="-1"></a>plt.plot(trace)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="2022-01-29-kl-divergence_files/figure-html/cell-50-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<div id="8037a211" class="cell" data-execution_count="355">
<div class="sourceCode cell-code" id="cb70"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb70-1"><a href="#cb70-1" aria-hidden="true" tabindex="-1"></a>loc, th_sample, scale</span>
<span id="cb70-2"><a href="#cb70-2" aria-hidden="true" tabindex="-1"></a><span class="co">#scale =  tfp.util.TransformedVariable([1., 1.], bijector=tfb.Exp())</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="355">
<pre><code>(&lt;tf.Variable 'Variable:0' shape=(2,) dtype=float32, numpy=array([2.0690155, 1.496477 ], dtype=float32)&gt;,
 &lt;tf.Tensor: shape=(2,), dtype=float32, numpy=array([1.5110626 , 0.42292204], dtype=float32)&gt;,
 &lt;TransformedVariable: name=soft_clip, dtype=float32, shape=[2], fn="soft_clip", numpy=array([0.5936036, 0.5403929], dtype=float32)&gt;)</code></pre>
</div>
</div>
<div id="47d71b31" class="cell" data-execution_count="354">
<div class="sourceCode cell-code" id="cb72"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb72-1"><a href="#cb72-1" aria-hidden="true" tabindex="-1"></a>plt.scatter(x[:, <span class="dv">0</span>], data_sample, s<span class="op">=</span><span class="dv">10</span>)</span>
<span id="cb72-2"><a href="#cb72-2" aria-hidden="true" tabindex="-1"></a>plt.plot(x[:, <span class="dv">0</span>], tf.reshape(x<span class="op">*</span>loc[<span class="dv">1</span>] <span class="op">+</span> loc[<span class="dv">0</span>], [<span class="op">-</span><span class="dv">1</span>]))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="2022-01-29-kl-divergence_files/figure-html/cell-52-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<div id="9abed5f2" class="cell" data-execution_count="359">
<div class="sourceCode cell-code" id="cb73"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb73-1"><a href="#cb73-1" aria-hidden="true" tabindex="-1"></a>loss_mc</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="359">
<pre><code>&lt;function __main__.loss_mc(loc, scale)&gt;</code></pre>
</div>
</div>
<div id="a467014d" class="cell" data-execution_count="372">
<div class="sourceCode cell-code" id="cb75"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb75-1"><a href="#cb75-1" aria-hidden="true" tabindex="-1"></a>loc <span class="op">=</span> tf.Variable([<span class="fl">1.0</span>, <span class="fl">0.4</span>], dtype<span class="op">=</span>tf.float32)</span>
<span id="cb75-2"><a href="#cb75-2" aria-hidden="true" tabindex="-1"></a>scale <span class="op">=</span> tfp.util.TransformedVariable([<span class="fl">.7</span>, <span class="fl">.6</span>], bijector<span class="op">=</span>tfb.SoftClip(<span class="fl">0.5</span>, <span class="fl">1.0</span>))</span>
<span id="cb75-3"><a href="#cb75-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb75-4"><a href="#cb75-4" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> loss_mc(loc, scale):</span>
<span id="cb75-5"><a href="#cb75-5" aria-hidden="true" tabindex="-1"></a>    q_to_learn <span class="op">=</span> tfd.Normal(loc<span class="op">=</span>loc, scale<span class="op">=</span>scale, name<span class="op">=</span><span class="st">"q_theta_learn"</span>)</span>
<span id="cb75-6"><a href="#cb75-6" aria-hidden="true" tabindex="-1"></a>    q_1 <span class="op">=</span> tfd.Normal(loc<span class="op">=</span>[<span class="fl">0.0</span>,<span class="fl">0.0</span>], scale<span class="op">=</span>[<span class="fl">1.0</span>, <span class="fl">1.0</span>])</span>
<span id="cb75-7"><a href="#cb75-7" aria-hidden="true" tabindex="-1"></a>    sample_set <span class="op">=</span> q_1.sample(n_samples)</span>
<span id="cb75-8"><a href="#cb75-8" aria-hidden="true" tabindex="-1"></a>    log_joint <span class="op">=</span> tf.reduce_sum(model.log_prob(sample_set, data_sample))</span>
<span id="cb75-9"><a href="#cb75-9" aria-hidden="true" tabindex="-1"></a>    log_q <span class="op">=</span> tf.reduce_sum(q_to_learn.log_prob(sample_set))</span>
<span id="cb75-10"><a href="#cb75-10" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> log_q <span class="op">-</span> log_joint</span>
<span id="cb75-11"><a href="#cb75-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb75-12"><a href="#cb75-12" aria-hidden="true" tabindex="-1"></a>target_log_prob_fn <span class="op">=</span> <span class="kw">lambda</span> th: model.log_prob((th, data_sample))</span>
<span id="cb75-13"><a href="#cb75-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb75-14"><a href="#cb75-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb75-15"><a href="#cb75-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb75-16"><a href="#cb75-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb75-17"><a href="#cb75-17" aria-hidden="true" tabindex="-1"></a>data_dim<span class="op">=</span><span class="dv">2</span></span>
<span id="cb75-18"><a href="#cb75-18" aria-hidden="true" tabindex="-1"></a>qt_mean <span class="op">=</span> tf.Variable(tf.random.normal([data_dim]))</span>
<span id="cb75-19"><a href="#cb75-19" aria-hidden="true" tabindex="-1"></a>qt_stddv <span class="op">=</span> tfp.util.TransformedVariable(</span>
<span id="cb75-20"><a href="#cb75-20" aria-hidden="true" tabindex="-1"></a>    <span class="fl">1e-4</span> <span class="op">*</span> tf.ones([data_dim]), bijector<span class="op">=</span>tfb.Softplus()</span>
<span id="cb75-21"><a href="#cb75-21" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb75-22"><a href="#cb75-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb75-23"><a href="#cb75-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb75-24"><a href="#cb75-24" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> factored_normal_variational_model():</span>
<span id="cb75-25"><a href="#cb75-25" aria-hidden="true" tabindex="-1"></a>    qt <span class="op">=</span> <span class="cf">yield</span> tfd.Normal(loc<span class="op">=</span>qt_mean, scale<span class="op">=</span>qt_stddv, name<span class="op">=</span><span class="st">"qt"</span>)</span>
<span id="cb75-26"><a href="#cb75-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb75-27"><a href="#cb75-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb75-28"><a href="#cb75-28" aria-hidden="true" tabindex="-1"></a>surrogate_posterior <span class="op">=</span> tfd.JointDistributionCoroutineAutoBatched(</span>
<span id="cb75-29"><a href="#cb75-29" aria-hidden="true" tabindex="-1"></a>    factored_normal_variational_model</span>
<span id="cb75-30"><a href="#cb75-30" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb75-31"><a href="#cb75-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb75-32"><a href="#cb75-32" aria-hidden="true" tabindex="-1"></a>losses <span class="op">=</span> tfp.vi.fit_surrogate_posterior(</span>
<span id="cb75-33"><a href="#cb75-33" aria-hidden="true" tabindex="-1"></a>    target_log_prob_fn<span class="op">=</span>target_log_prob_fn,</span>
<span id="cb75-34"><a href="#cb75-34" aria-hidden="true" tabindex="-1"></a>    surrogate_posterior<span class="op">=</span>surrogate_posterior,</span>
<span id="cb75-35"><a href="#cb75-35" aria-hidden="true" tabindex="-1"></a>    optimizer<span class="op">=</span>tf.optimizers.Adam(learning_rate<span class="op">=</span><span class="fl">0.05</span>),</span>
<span id="cb75-36"><a href="#cb75-36" aria-hidden="true" tabindex="-1"></a>    num_steps<span class="op">=</span><span class="dv">100</span>,</span>
<span id="cb75-37"><a href="#cb75-37" aria-hidden="true" tabindex="-1"></a>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>/Users/nipun/miniforge3/lib/python3.9/site-packages/tensorflow_probability/python/internal/vectorization_util.py:87: UserWarning: Saw Tensor seed Tensor("seed:0", shape=(2,), dtype=int32), implying stateless sampling. Autovectorized functions that use stateless sampling may be quite slow because the current implementation falls back to an explicit loop. This will be fixed in the future. For now, you will likely see better performance from stateful sampling, which you can invoke by passing a Python `int` seed.
  warnings.warn(
/Users/nipun/miniforge3/lib/python3.9/site-packages/tensorflow_probability/python/internal/vectorization_util.py:87: UserWarning: Saw Tensor seed Tensor("seed:0", shape=(2,), dtype=int32), implying stateless sampling. Autovectorized functions that use stateless sampling may be quite slow because the current implementation falls back to an explicit loop. This will be fixed in the future. For now, you will likely see better performance from stateful sampling, which you can invoke by passing a Python `int` seed.
  warnings.warn(</code></pre>
</div>
</div>
<div id="d177c69b" class="cell" data-execution_count="373">
<div class="sourceCode cell-code" id="cb77"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb77-1"><a href="#cb77-1" aria-hidden="true" tabindex="-1"></a>plt.plot(losses)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="2022-01-29-kl-divergence_files/figure-html/cell-55-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<div id="18e867e7" class="cell" data-execution_count="376">
<div class="sourceCode cell-code" id="cb78"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="376">
<pre><code>StructTuple(
  qt=&lt;tf.Tensor: shape=(2,), dtype=float32, numpy=array([1.5720905, 0.4626296], dtype=float32)&gt;
)</code></pre>
</div>
</div>
<div id="3bd943d5" class="cell" data-execution_count="374">
<div class="sourceCode cell-code" id="cb80"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb80-1"><a href="#cb80-1" aria-hidden="true" tabindex="-1"></a>qt_mean, qt_stddv</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="374">
<pre><code>(&lt;tf.Variable 'Variable:0' shape=(2,) dtype=float32, numpy=array([1.5777218 , 0.46246716], dtype=float32)&gt;,
 &lt;TransformedVariable: name=softplus, dtype=float32, shape=[2], fn="softplus", numpy=array([0.01456336, 0.01367522], dtype=float32)&gt;)</code></pre>
</div>
</div>
<div id="df4543be" class="cell" data-execution_count="375">
<div class="sourceCode cell-code" id="cb82"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb82-1"><a href="#cb82-1" aria-hidden="true" tabindex="-1"></a>plt.scatter(x[:, <span class="dv">0</span>], data_sample, s<span class="op">=</span><span class="dv">10</span>)</span>
<span id="cb82-2"><a href="#cb82-2" aria-hidden="true" tabindex="-1"></a>plt.plot(x[:, <span class="dv">0</span>], tf.reshape(x<span class="op">*</span>qt_mean[<span class="dv">1</span>] <span class="op">+</span> qt_mean[<span class="dv">0</span>], [<span class="op">-</span><span class="dv">1</span>]))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="2022-01-29-kl-divergence_files/figure-html/cell-58-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<div id="3eca3005" class="cell" data-execution_count="393">
<div class="sourceCode cell-code" id="cb83"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb83-1"><a href="#cb83-1" aria-hidden="true" tabindex="-1"></a>post_samples <span class="op">=</span> surrogate_posterior.sample(<span class="dv">200</span>)</span>
<span id="cb83-2"><a href="#cb83-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb83-3"><a href="#cb83-3" aria-hidden="true" tabindex="-1"></a>post_samples.qt[<span class="dv">0</span>:<span class="dv">5</span>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>WARNING:tensorflow:Note that RandomStandardNormal inside pfor op may not give same output as inside a sequential loop.</code></pre>
</div>
<div class="cell-output cell-output-display" data-execution_count="393">
<pre><code>&lt;tf.Tensor: shape=(5, 2), dtype=float32, numpy=
array([[1.5795265 , 0.500741  ],
       [1.5515635 , 0.46671686],
       [1.5585055 , 0.4617632 ],
       [1.5856469 , 0.44141397],
       [1.5763292 , 0.45420292]], dtype=float32)&gt;</code></pre>
</div>
</div>
<div id="7d21083a" class="cell" data-execution_count="394">
<div class="sourceCode cell-code" id="cb86"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb86-1"><a href="#cb86-1" aria-hidden="true" tabindex="-1"></a>plt.scatter(x[:, <span class="dv">0</span>], data_sample, s<span class="op">=</span><span class="dv">10</span>)</span>
<span id="cb86-2"><a href="#cb86-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb86-3"><a href="#cb86-3" aria-hidden="true" tabindex="-1"></a>plt.plot(x[:, <span class="dv">0</span>], tf.reshape(x<span class="op">*</span>qt_mean[<span class="dv">1</span>] <span class="op">+</span> qt_mean[<span class="dv">0</span>], [<span class="op">-</span><span class="dv">1</span>]))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="2022-01-29-kl-divergence_files/figure-html/cell-60-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>References</p>
<ul>
<li>https://www.youtube.com/watch?v=HUsznqt2V5I</li>
<li>https://www.youtube.com/watch?v=x9StQ8RZ0ag&amp;list=PLISXH-iEM4JlFsAp7trKCWyxeO3M70QyJ&amp;index=9</li>
<li>https://colab.research.google.com/github/goodboychan/goodboychan.github.io/blob/main/_notebooks/2021-09-13-02-Minimizing-KL-Divergence.ipynb#scrollTo=gd_ev8ceII8q</li>
<li>https://goodboychan.github.io/python/coursera/tensorflow_probability/icl/2021/09/13/02-Minimizing-KL-Divergence.html</li>
</ul>


</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const disableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'prefetch';
    }
  }
  const enableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'stylesheet';
    }
  }
  const manageTransitions = (selector, allowTransitions) => {
    const els = window.document.querySelectorAll(selector);
    for (let i=0; i < els.length; i++) {
      const el = els[i];
      if (allowTransitions) {
        el.classList.remove('notransition');
      } else {
        el.classList.add('notransition');
      }
    }
  }
  const toggleGiscusIfUsed = (isAlternate, darkModeDefault) => {
    const baseTheme = document.querySelector('#giscus-base-theme')?.value ?? 'light';
    const alternateTheme = document.querySelector('#giscus-alt-theme')?.value ?? 'dark';
    let newTheme = '';
    if(darkModeDefault) {
      newTheme = isAlternate ? baseTheme : alternateTheme;
    } else {
      newTheme = isAlternate ? alternateTheme : baseTheme;
    }
    const changeGiscusTheme = () => {
      // From: https://github.com/giscus/giscus/issues/336
      const sendMessage = (message) => {
        const iframe = document.querySelector('iframe.giscus-frame');
        if (!iframe) return;
        iframe.contentWindow.postMessage({ giscus: message }, 'https://giscus.app');
      }
      sendMessage({
        setConfig: {
          theme: newTheme
        }
      });
    }
    const isGiscussLoaded = window.document.querySelector('iframe.giscus-frame') !== null;
    if (isGiscussLoaded) {
      changeGiscusTheme();
    }
  }
  const toggleColorMode = (alternate) => {
    // Switch the stylesheets
    const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
    manageTransitions('#quarto-margin-sidebar .nav-link', false);
    if (alternate) {
      enableStylesheet(alternateStylesheets);
      for (const sheetNode of alternateStylesheets) {
        if (sheetNode.id === "quarto-bootstrap") {
          toggleBodyColorMode(sheetNode);
        }
      }
    } else {
      disableStylesheet(alternateStylesheets);
      toggleBodyColorPrimary();
    }
    manageTransitions('#quarto-margin-sidebar .nav-link', true);
    // Switch the toggles
    const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
    for (let i=0; i < toggles.length; i++) {
      const toggle = toggles[i];
      if (toggle) {
        if (alternate) {
          toggle.classList.add("alternate");     
        } else {
          toggle.classList.remove("alternate");
        }
      }
    }
    // Hack to workaround the fact that safari doesn't
    // properly recolor the scrollbar when toggling (#1455)
    if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
      manageTransitions("body", false);
      window.scrollTo(0, 1);
      setTimeout(() => {
        window.scrollTo(0, 0);
        manageTransitions("body", true);
      }, 40);  
    }
  }
  const isFileUrl = () => { 
    return window.location.protocol === 'file:';
  }
  const hasAlternateSentinel = () => {  
    let styleSentinel = getColorSchemeSentinel();
    if (styleSentinel !== null) {
      return styleSentinel === "alternate";
    } else {
      return false;
    }
  }
  const setStyleSentinel = (alternate) => {
    const value = alternate ? "alternate" : "default";
    if (!isFileUrl()) {
      window.localStorage.setItem("quarto-color-scheme", value);
    } else {
      localAlternateSentinel = value;
    }
  }
  const getColorSchemeSentinel = () => {
    if (!isFileUrl()) {
      const storageValue = window.localStorage.getItem("quarto-color-scheme");
      return storageValue != null ? storageValue : localAlternateSentinel;
    } else {
      return localAlternateSentinel;
    }
  }
  const darkModeDefault = false;
  let localAlternateSentinel = darkModeDefault ? 'alternate' : 'default';
  // Dark / light mode switch
  window.quartoToggleColorScheme = () => {
    // Read the current dark / light value 
    let toAlternate = !hasAlternateSentinel();
    toggleColorMode(toAlternate);
    setStyleSentinel(toAlternate);
    toggleGiscusIfUsed(toAlternate, darkModeDefault);
  };
  // Ensure there is a toggle, if there isn't float one in the top right
  if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
    const a = window.document.createElement('a');
    a.classList.add('top-right');
    a.classList.add('quarto-color-scheme-toggle');
    a.href = "";
    a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
    const i = window.document.createElement("i");
    i.classList.add('bi');
    a.appendChild(i);
    window.document.body.appendChild(a);
  }
  // Switch to dark mode if need be
  if (hasAlternateSentinel()) {
    toggleColorMode(true);
  } else {
    toggleColorMode(false);
  }
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp("https:\/\/nipunbatra\.github\.io\/blog\/");
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->




</body></html>
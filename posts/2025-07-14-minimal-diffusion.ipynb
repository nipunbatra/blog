{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "Diffusion models have become one of the most powerful approaches for generative modeling, powering everything from DALL-E to Stable Diffusion. At their core, they work by gradually adding noise to data and then learning to reverse this process. \n\nThe key insight is surprisingly simple: if we can learn to denoise images at different noise levels, we can generate new images by starting with pure noise and progressively denoising it. This tutorial implements a minimal diffusion model using PyTorch and the sklearn digits dataset.\n\n## The Math Behind Diffusion\n\nThe diffusion process consists of two parts:\n\n**Forward Process (Adding Noise):** We gradually add Gaussian noise to clean images over $T$ timesteps:\n$$q(x_t | x_{t-1}) = \\mathcal{N}(x_t; \\sqrt{1-\\beta_t} x_{t-1}, \\beta_t I)$$\n\n**Reverse Process (Denoising):** We learn to reverse this process:\n$$p_\\theta(x_{t-1} | x_t) = \\mathcal{N}(x_{t-1}; \\mu_\\theta(x_t, t), \\sigma_t^2 I)$$\n\nThe beautiful mathematical insight is that we can train a neural network to predict the noise $\\epsilon$ that was added, rather than directly predicting the clean image."
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib.animation as animation\nfrom sklearn.datasets import load_digits\nfrom torch.utils.data import DataLoader, TensorDataset\nfrom tqdm import tqdm\nimport seaborn as sns\nfrom IPython.display import HTML\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Set random seed for reproducibility\ntorch.manual_seed(42)\nnp.random.seed(42)\n\n# Set up plotting style\nplt.style.use('seaborn-v0_8')\nsns.set_palette(\"husl\")"
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "## Load and Explore the Data\n\nWe'll use the sklearn digits dataset, which contains 8×8 grayscale images of handwritten digits 0-9. This is perfect for our minimal implementation as the images are small and the dataset is manageable.\n\nThe dataset contains 1,797 samples, each representing a digit as a 64-dimensional vector (8×8 flattened). We'll normalize the pixel values to the range [0, 1] to match the assumptions of our diffusion model."
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# Load and prepare the digits dataset\ndigits = load_digits()\nX = digits.data.reshape(-1, 8, 8)  # 8x8 images\nX = X / 16.0  # Normalize to [0, 1] (original max value is 16)\n\n# Convert to PyTorch tensors\nX_tensor = torch.FloatTensor(X).unsqueeze(1)  # Add channel dimension: [N, 1, 8, 8]\n\n# Create DataLoader\ndataset = TensorDataset(X_tensor)\ndataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n\nprint(f\"Dataset shape: {X_tensor.shape}\")\nprint(f\"Data range: [{X_tensor.min():.3f}, {X_tensor.max():.3f}]\")\nprint(f\"Number of samples: {len(X_tensor)}\")\nprint(f\"Unique digits: {np.unique(digits.target)}\")",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Show overall data distribution\nplt.figure(figsize=(12, 4))\n\nplt.subplot(1, 3, 1)\nplt.hist(digits.target, bins=10, alpha=0.7, color='skyblue', edgecolor='black')\nplt.title('Distribution of Digits')\nplt.xlabel('Digit')\nplt.ylabel('Count')\n\nplt.subplot(1, 3, 2)\nplt.hist(X.flatten(), bins=50, alpha=0.7, color='lightgreen', edgecolor='black')\nplt.title('Pixel Intensity Distribution')\nplt.xlabel('Pixel Value')\nplt.ylabel('Count')\n\nplt.subplot(1, 3, 3)\nmean_image = X.mean(axis=0)\nplt.imshow(mean_image, cmap='gray')\nplt.title('Mean Image Across Dataset')\nplt.colorbar()\nplt.axis('off')\n\nplt.tight_layout()\nplt.show()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Define the diffusion scheduler\nclass DiffusionScheduler:\n    def __init__(self, num_timesteps=1000, beta_start=1e-4, beta_end=0.02):\n        \"\"\"\n        Initialize the diffusion scheduler with a linear beta schedule.\n        \n        Args:\n            num_timesteps: Number of diffusion steps\n            beta_start: Starting value for beta schedule\n            beta_end: Ending value for beta schedule\n        \"\"\"\n        self.num_timesteps = num_timesteps\n        \n        # Linear schedule for beta values\n        self.betas = torch.linspace(beta_start, beta_end, num_timesteps)\n        self.alphas = 1.0 - self.betas\n        self.alphas_cumprod = torch.cumprod(self.alphas, dim=0)\n        \n        # Useful for sampling\n        self.alphas_cumprod_prev = F.pad(self.alphas_cumprod[:-1], (1, 0), value=1.0)\n        \n    def add_noise(self, x0, t, noise=None):\n        \"\"\"\n        Add noise to clean images at timestep t using the reparameterization trick.\n        \n        Args:\n            x0: Clean images [batch_size, channels, height, width]\n            t: Timesteps [batch_size]\n            noise: Optional noise tensor, will be generated if None\n            \n        Returns:\n            Noisy images at timestep t\n        \"\"\"\n        if noise is None:\n            noise = torch.randn_like(x0)\n        \n        sqrt_alphas_cumprod = torch.sqrt(self.alphas_cumprod[t])\n        sqrt_one_minus_alphas_cumprod = torch.sqrt(1 - self.alphas_cumprod[t])\n        \n        # Reshape for broadcasting\n        sqrt_alphas_cumprod = sqrt_alphas_cumprod.view(-1, 1, 1, 1)\n        sqrt_one_minus_alphas_cumprod = sqrt_one_minus_alphas_cumprod.view(-1, 1, 1, 1)\n        \n        return sqrt_alphas_cumprod * x0 + sqrt_one_minus_alphas_cumprod * noise\n\nscheduler = DiffusionScheduler()\nprint(f\"Scheduler initialized with {scheduler.num_timesteps} timesteps\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Demonstrate the forward diffusion process\nsample_image = X_tensor[0:1]  # Take first image\ntimesteps = [0, 50, 100, 200, 400, 600, 800, 999]\n\nfig, axes = plt.subplots(2, len(timesteps), figsize=(20, 6))\nfig.suptitle('Forward Diffusion Process: Gradual Noise Addition', fontsize=16)\n\nfor i, t in enumerate(timesteps):\n    t_tensor = torch.tensor([t])\n    noisy_image = scheduler.add_noise(sample_image, t_tensor)\n    \n    # Show image\n    axes[0, i].imshow(noisy_image[0, 0].numpy(), cmap='gray')\n    axes[0, i].set_title(f't = {t}')\n    axes[0, i].axis('off')\n    \n    # Show histogram of pixel values\n    axes[1, i].hist(noisy_image[0, 0].flatten().numpy(), bins=20, alpha=0.7, color=f'C{i}')\n    axes[1, i].set_title(f'Histogram t={t}')\n    axes[1, i].set_xlim([-3, 3])\n    axes[1, i].set_ylim([0, 20])\n\nplt.tight_layout()\nplt.show()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Time embedding using sinusoidal positional encoding\nclass TimeEmbedding(nn.Module):\n    \"\"\"Sinusoidal time embedding similar to transformers\"\"\"\n    def __init__(self, embed_dim):\n        super().__init__()\n        self.embed_dim = embed_dim\n        \n    def forward(self, t):\n        device = t.device\n        half_dim = self.embed_dim // 2\n        embeddings = np.log(10000) / (half_dim - 1)\n        embeddings = torch.exp(torch.arange(half_dim, device=device) * -embeddings)\n        embeddings = t[:, None] * embeddings[None, :]\n        embeddings = torch.cat([torch.sin(embeddings), torch.cos(embeddings)], dim=-1)\n        return embeddings",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Create and test the model\nmodel = ImprovedDenoiser()\ntotal_params = sum(p.numel() for p in model.parameters())\nprint(f\"Model created with {total_params:,} parameters\")\n\n# Test the model\ntest_input = torch.randn(2, 1, 8, 8)\ntest_time = torch.randint(0, 1000, (2,))\ntest_output = model(test_input, test_time)\nprint(f\"Test input shape: {test_input.shape}\")\nprint(f\"Test output shape: {test_output.shape}\")\nprint(f\"Model successfully processes input → output\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Training setup\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nmodel = model.to(device)\nscheduler.betas = scheduler.betas.to(device)\nscheduler.alphas = scheduler.alphas.to(device)\nscheduler.alphas_cumprod = scheduler.alphas_cumprod.to(device)\n\noptimizer = optim.AdamW(model.parameters(), lr=2e-4, weight_decay=0.01)\ncriterion = nn.MSELoss()\n\nnum_epochs = 50\nlosses = []\nepoch_losses = []\n\nprint(f\"Training on device: {device}\")\nprint(f\"Training for {num_epochs} epochs with {len(dataloader)} batches per epoch\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Plot training progress\nfig, axes = plt.subplots(1, 3, figsize=(18, 5))\n\n# Loss over batches\naxes[0].plot(losses, alpha=0.7)\naxes[0].set_title('Training Loss (per batch)')\naxes[0].set_xlabel('Batch')\naxes[0].set_ylabel('MSE Loss')\naxes[0].grid(True)\n\n# Loss over epochs\naxes[1].plot(epoch_losses, 'o-')\naxes[1].set_title('Training Loss (per epoch)')\naxes[1].set_xlabel('Epoch')\naxes[1].set_ylabel('Average MSE Loss')\naxes[1].grid(True)\n\n# Loss distribution\naxes[2].hist(losses, bins=50, alpha=0.7, edgecolor='black')\naxes[2].set_title('Distribution of Batch Losses')\naxes[2].set_xlabel('Loss')\naxes[2].set_ylabel('Frequency')\naxes[2].grid(True)\n\nplt.tight_layout()\nplt.show()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Define sampling function\ndef sample_images(model, scheduler, num_samples=8, device='cpu', show_progress=True):\n    \"\"\"Generate images using the trained diffusion model\"\"\"\n    model.eval()\n    \n    with torch.no_grad():\n        # Start with pure noise\n        x = torch.randn(num_samples, 1, 8, 8, device=device)\n        \n        # Store intermediate steps for visualization\n        intermediate_steps = []\n        save_steps = [999, 800, 600, 400, 200, 100, 50, 0]\n        \n        # Reverse diffusion process\n        iterator = reversed(range(scheduler.num_timesteps))\n        if show_progress:\n            iterator = tqdm(iterator, desc=\"Sampling\")\n            \n        for i, t in enumerate(iterator):\n            t_tensor = torch.full((num_samples,), t, device=device)\n            \n            # Predict noise\n            predicted_noise = model(x, t_tensor)\n            \n            # Remove predicted noise\n            alpha_t = scheduler.alphas[t]\n            alpha_cumprod_t = scheduler.alphas_cumprod[t]\n            beta_t = scheduler.betas[t]\n            \n            # Compute x_{t-1}\n            x = (1 / torch.sqrt(alpha_t)) * (x - (beta_t / torch.sqrt(1 - alpha_cumprod_t)) * predicted_noise)\n            \n            # Add noise if not the last step\n            if t > 0:\n                noise = torch.randn_like(x)\n                x = x + torch.sqrt(beta_t) * noise\n            \n            # Save intermediate steps\n            if t in save_steps:\n                intermediate_steps.append((t, x.clone().cpu()))\n    \n    return x.cpu(), intermediate_steps",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Show quality metrics\nprint(\"Generated Image Statistics:\")\nprint(f\"Mean pixel value: {generated_images.mean():.3f}\")\nprint(f\"Std pixel value: {generated_images.std():.3f}\")\nprint(f\"Min pixel value: {generated_images.min():.3f}\")\nprint(f\"Max pixel value: {generated_images.max():.3f}\")\n\nprint(\"\\nOriginal Image Statistics:\")\nprint(f\"Mean pixel value: {X_tensor.mean():.3f}\")\nprint(f\"Std pixel value: {X_tensor.std():.3f}\")\nprint(f\"Min pixel value: {X_tensor.min():.3f}\")\nprint(f\"Max pixel value: {X_tensor.max():.3f}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Visualize the reverse diffusion process\nintermediates = all_intermediates[0]  # Use first batch's intermediates\n\nfig, axes = plt.subplots(len(intermediates), 8, figsize=(16, 20))\nfig.suptitle('Reverse Diffusion Process: From Noise to Digits', fontsize=16)\n\nfor step_idx, (timestep, images) in enumerate(intermediates):\n    for sample_idx in range(8):\n        axes[step_idx, sample_idx].imshow(images[sample_idx, 0], cmap='gray')\n        axes[step_idx, sample_idx].axis('off')\n        if sample_idx == 0:\n            axes[step_idx, sample_idx].set_ylabel(f't = {timestep}', fontsize=12)\n        if step_idx == 0:\n            axes[step_idx, sample_idx].set_title(f'Sample {sample_idx+1}', fontsize=10)\n\nplt.tight_layout()\nplt.show()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Show pixel intensity evolution\nfig, axes = plt.subplots(1, 2, figsize=(15, 5))\n\n# Plot mean pixel intensity over time\nmean_intensities = []\nstd_intensities = []\ntimesteps_list = []\n\nfor timestep, images in intermediates:\n    mean_intensities.append(images.mean().item())\n    std_intensities.append(images.std().item())\n    timesteps_list.append(timestep)\n\naxes[0].plot(timesteps_list, mean_intensities, 'o-', label='Mean Intensity')\naxes[0].fill_between(timesteps_list, \n                     np.array(mean_intensities) - np.array(std_intensities),\n                     np.array(mean_intensities) + np.array(std_intensities),\n                     alpha=0.3)\naxes[0].set_xlabel('Timestep')\naxes[0].set_ylabel('Pixel Intensity')\naxes[0].set_title('Pixel Intensity Evolution During Sampling')\naxes[0].legend()\naxes[0].grid(True)\n\n# Show histogram evolution\nsample_images = [images[0, 0].numpy().flatten() for _, images in intermediates]\ncolors = plt.cm.viridis(np.linspace(0, 1, len(sample_images)))\n\nfor i, (pixels, color) in enumerate(zip(sample_images, colors)):\n    axes[1].hist(pixels, bins=30, alpha=0.5, color=color, \n                label=f't={timesteps_list[i]}' if i % 2 == 0 else '')\n\naxes[1].set_xlabel('Pixel Value')\naxes[1].set_ylabel('Frequency')\naxes[1].set_title('Pixel Value Distribution Evolution')\naxes[1].legend()\naxes[1].grid(True)\n\nplt.tight_layout()\nplt.show()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Create detailed animation function\ndef create_detailed_animation():\n    \"\"\"Create detailed animation frames showing the full reverse process\"\"\"\n    model.eval()\n    \n    with torch.no_grad():\n        # Start with pure noise (single sample for cleaner animation)\n        x = torch.randn(1, 1, 8, 8, device=device)\n        \n        # Store ALL intermediate steps\n        all_frames = []\n        \n        # Reverse diffusion process\n        for t in tqdm(reversed(range(scheduler.num_timesteps)), desc=\"Creating animation\"):\n            # Save every 50th frame to keep animation manageable\n            if t % 50 == 0 or t < 50:\n                all_frames.append((t, x[0, 0].cpu().numpy().copy()))\n            \n            t_tensor = torch.full((1,), t, device=device)\n            \n            # Predict noise\n            predicted_noise = model(x, t_tensor)\n            \n            # Remove predicted noise\n            alpha_t = scheduler.alphas[t]\n            alpha_cumprod_t = scheduler.alphas_cumprod[t]\n            beta_t = scheduler.betas[t]\n            \n            # Compute x_{t-1}\n            x = (1 / torch.sqrt(alpha_t)) * (x - (beta_t / torch.sqrt(1 - alpha_cumprod_t)) * predicted_noise)\n            \n            # Add noise if not the last step\n            if t > 0:\n                noise = torch.randn_like(x)\n                x = x + torch.sqrt(beta_t) * noise\n    \n    return all_frames\n\n# Create animation frames\nprint(\"Creating detailed animation frames...\")\nanimation_frames = create_detailed_animation()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Show final comparison\nfig, axes = plt.subplots(len(comparison_frames), 4, figsize=(12, 20))\nfig.suptitle('Multiple Samples Diffusion Process', fontsize=16)\n\nfor step_idx, (timestep, images) in enumerate(comparison_frames):\n    for sample_idx in range(4):\n        axes[step_idx, sample_idx].imshow(images[sample_idx, 0], cmap='gray')\n        axes[step_idx, sample_idx].axis('off')\n        if sample_idx == 0:\n            axes[step_idx, sample_idx].set_ylabel(f't = {timestep}', fontsize=10)\n        if step_idx == 0:\n            axes[step_idx, sample_idx].set_title(f'Sample {sample_idx+1}', fontsize=10)\n\nplt.tight_layout()\nplt.show()\n\nprint(f\"Created {len(animation_frames)} animation frames\")\nprint(f\"Created {len(comparison_frames)} comparison frames\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Show key frames from the animation\nkey_frames = animation_frames[::len(animation_frames)//8]  # Show 8 key frames\nfig, axes = plt.subplots(1, len(key_frames), figsize=(20, 3))\nfig.suptitle('Key Frames from Diffusion Animation', fontsize=16)\n\nfor i, (timestep, image) in enumerate(key_frames):\n    axes[i].imshow(image, cmap='gray')\n    axes[i].set_title(f't = {timestep}')\n    axes[i].axis('off')\n\nplt.tight_layout()\nplt.show()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Create and save animation\nfig, ax = plt.subplots(figsize=(6, 6))\nax.set_xlim(0, 8)\nax.set_ylim(0, 8)\nax.set_aspect('equal')\n\ndef animate(frame_idx):\n    ax.clear()\n    timestep, image = animation_frames[frame_idx]\n    \n    # Display the image\n    im = ax.imshow(image, cmap='gray', vmin=-2, vmax=2)\n    ax.set_title(f'Diffusion Sampling: t = {timestep}', fontsize=14)\n    ax.axis('off')\n    \n    return [im]\n\n# Create animation\nanim = animation.FuncAnimation(fig, animate, frames=len(animation_frames), \n                             interval=200, blit=False, repeat=True)\n\n# Save as GIF\nprint(\"Saving animation as GIF...\")\nanim.save('diffusion_process.gif', writer='pillow', fps=5)\nprint(\"Animation saved as 'diffusion_process.gif'\")\n\n# Display the animation in notebook\nplt.show()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Show the progression for a single sample\nsample_idx = 0\nfig, axes = plt.subplots(1, len(intermediates), figsize=(20, 3))\nfig.suptitle(f'Single Sample Progression: From Noise to Digit', fontsize=16)\n\nfor step_idx, (timestep, images) in enumerate(intermediates):\n    axes[step_idx].imshow(images[sample_idx, 0], cmap='gray')\n    axes[step_idx].set_title(f't = {timestep}')\n    axes[step_idx].axis('off')\n\nplt.tight_layout()\nplt.show()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Show multiple generations\nfig, axes = plt.subplots(5, 8, figsize=(16, 10))\nfig.suptitle('Multiple Generated Samples (5 batches × 8 samples)', fontsize=16)\n\nfor batch_idx in range(5):\n    for sample_idx in range(8):\n        img_idx = batch_idx * 8 + sample_idx\n        axes[batch_idx, sample_idx].imshow(generated_images[img_idx, 0], cmap='gray')\n        axes[batch_idx, sample_idx].axis('off')\n        axes[batch_idx, sample_idx].set_title(f'B{batch_idx+1}S{sample_idx+1}', fontsize=8)\n\nplt.tight_layout()\nplt.show()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Generate multiple batches of samples\nprint(\"Generating samples...\")\nall_samples = []\nall_intermediates = []\n\nfor i in range(5):  # Generate 5 batches of 8 samples each\n    samples, intermediates = sample_images(model, scheduler, num_samples=8, device=device, show_progress=(i==0))\n    all_samples.append(samples)\n    all_intermediates.append(intermediates)\n    print(f\"Generated batch {i+1}/5\")\n\n# Combine all samples\ngenerated_images = torch.cat(all_samples, dim=0)\nprint(f\"Generated {len(generated_images)} total images\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Self-attention mechanism\nclass AttentionBlock(nn.Module):\n    \"\"\"Simple self-attention block\"\"\"\n    def __init__(self, channels):\n        super().__init__()\n        self.channels = channels\n        self.norm = nn.GroupNorm(8, channels)\n        self.qkv = nn.Conv2d(channels, channels * 3, 1)\n        self.proj = nn.Conv2d(channels, channels, 1)\n        \n    def forward(self, x):\n        b, c, h, w = x.shape\n        x_norm = self.norm(x)\n        qkv = self.qkv(x_norm)\n        q, k, v = qkv.chunk(3, dim=1)\n        \n        # Reshape for attention\n        q = q.view(b, c, h * w).transpose(1, 2)\n        k = k.view(b, c, h * w).transpose(1, 2)\n        v = v.view(b, c, h * w).transpose(1, 2)\n        \n        # Compute attention\n        attn = torch.softmax(q @ k.transpose(-2, -1) / (c ** 0.5), dim=-1)\n        out = attn @ v\n        \n        # Reshape back\n        out = out.transpose(1, 2).view(b, c, h, w)\n        out = self.proj(out)\n        \n        return x + out",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Residual block with time conditioning\nclass ResidualBlock(nn.Module):\n    \"\"\"Residual block with time conditioning\"\"\"\n    def __init__(self, in_channels, out_channels, time_embed_dim):\n        super().__init__()\n        self.conv1 = nn.Conv2d(in_channels, out_channels, 3, padding=1)\n        self.conv2 = nn.Conv2d(out_channels, out_channels, 3, padding=1)\n        self.time_mlp = nn.Sequential(\n            nn.Linear(time_embed_dim, out_channels),\n            nn.ReLU()\n        )\n        self.norm1 = nn.GroupNorm(8, out_channels)\n        self.norm2 = nn.GroupNorm(8, out_channels)\n        self.shortcut = nn.Conv2d(in_channels, out_channels, 1) if in_channels != out_channels else nn.Identity()\n        \n    def forward(self, x, time_emb):\n        # First convolution\n        h = self.conv1(x)\n        h = self.norm1(h)\n        h = F.relu(h)\n        \n        # Add time conditioning\n        time_cond = self.time_mlp(time_emb)\n        h = h + time_cond.unsqueeze(-1).unsqueeze(-1)\n        \n        # Second convolution\n        h = self.conv2(h)\n        h = self.norm2(h)\n        h = F.relu(h)\n        \n        # Skip connection\n        return h + self.shortcut(x)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "## Forward Diffusion Process: Adding Noise\n\nThe forward diffusion process gradually corrupts clean images by adding Gaussian noise. The key insight is that we can jump to any timestep $t$ directly using the reparameterization trick:\n\n$$x_t = \\sqrt{\\bar{\\alpha}_t} x_0 + \\sqrt{1 - \\bar{\\alpha}_t} \\epsilon$$\n\nwhere:\n- $x_0$ is the clean image\n- $\\epsilon \\sim \\mathcal{N}(0, I)$ is random noise\n- $\\bar{\\alpha}_t = \\prod_{i=1}^t \\alpha_i$ is the cumulative product of alphas\n- $\\alpha_t = 1 - \\beta_t$ where $\\beta_t$ is the noise schedule\n\nThis allows us to efficiently sample noisy images at any timestep during training without having to iteratively apply the forward process."
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "class DiffusionScheduler:\n    def __init__(self, num_timesteps=1000, beta_start=1e-4, beta_end=0.02):\n        \"\"\"\n        Initialize the diffusion scheduler with a linear beta schedule.\n        \n        Args:\n            num_timesteps: Number of diffusion steps\n            beta_start: Starting value for beta schedule\n            beta_end: Ending value for beta schedule\n        \"\"\"\n        self.num_timesteps = num_timesteps\n        \n        # Linear schedule for beta values\n        self.betas = torch.linspace(beta_start, beta_end, num_timesteps)\n        self.alphas = 1.0 - self.betas\n        self.alphas_cumprod = torch.cumprod(self.alphas, dim=0)\n        \n        # Useful for sampling\n        self.alphas_cumprod_prev = F.pad(self.alphas_cumprod[:-1], (1, 0), value=1.0)\n        \n    def add_noise(self, x0, t, noise=None):\n        \"\"\"\n        Add noise to clean images at timestep t using the reparameterization trick.\n        \n        Args:\n            x0: Clean images [batch_size, channels, height, width]\n            t: Timesteps [batch_size]\n            noise: Optional noise tensor, will be generated if None\n            \n        Returns:\n            Noisy images at timestep t\n        \"\"\"\n        if noise is None:\n            noise = torch.randn_like(x0)\n        \n        sqrt_alphas_cumprod = torch.sqrt(self.alphas_cumprod[t])\n        sqrt_one_minus_alphas_cumprod = torch.sqrt(1 - self.alphas_cumprod[t])\n        \n        # Reshape for broadcasting\n        sqrt_alphas_cumprod = sqrt_alphas_cumprod.view(-1, 1, 1, 1)\n        sqrt_one_minus_alphas_cumprod = sqrt_one_minus_alphas_cumprod.view(-1, 1, 1, 1)\n        \n        return sqrt_alphas_cumprod * x0 + sqrt_one_minus_alphas_cumprod * noise\n\nscheduler = DiffusionScheduler()\nprint(f\"Scheduler initialized with {scheduler.num_timesteps} timesteps\")\n\n# Visualize the noise schedule\nfig, axes = plt.subplots(1, 3, figsize=(15, 4))\n\n# Beta schedule\naxes[0].plot(scheduler.betas.numpy())\naxes[0].set_title('Beta Schedule (Noise Rate)')\naxes[0].set_xlabel('Timestep')\naxes[0].set_ylabel('Beta')\naxes[0].grid(True)\n\n# Alpha schedule\naxes[1].plot(scheduler.alphas.numpy())\naxes[1].set_title('Alpha Schedule (1 - Beta)')\naxes[1].set_xlabel('Timestep')\naxes[1].set_ylabel('Alpha')\naxes[1].grid(True)\n\n# Cumulative alpha schedule\naxes[2].plot(scheduler.alphas_cumprod.numpy())\naxes[2].set_title('Cumulative Alpha Schedule')\naxes[2].set_xlabel('Timestep')\naxes[2].set_ylabel('Cumulative Alpha')\naxes[2].grid(True)\n\nplt.tight_layout()\nplt.show()\n\n# Show the forward diffusion process on a sample image\nsample_image = X_tensor[0:1]  # Take first image\ntimesteps = [0, 50, 100, 200, 400, 600, 800, 999]\n\nfig, axes = plt.subplots(2, len(timesteps), figsize=(20, 6))\nfig.suptitle('Forward Diffusion Process: Gradual Noise Addition', fontsize=16)\n\nfor i, t in enumerate(timesteps):\n    t_tensor = torch.tensor([t])\n    noisy_image = scheduler.add_noise(sample_image, t_tensor)\n    \n    # Show image\n    axes[0, i].imshow(noisy_image[0, 0].numpy(), cmap='gray')\n    axes[0, i].set_title(f't = {t}')\n    axes[0, i].axis('off')\n    \n    # Show histogram of pixel values\n    axes[1, i].hist(noisy_image[0, 0].flatten().numpy(), bins=20, alpha=0.7, color=f'C{i}')\n    axes[1, i].set_title(f'Histogram t={t}')\n    axes[1, i].set_xlim([-3, 3])\n    axes[1, i].set_ylim([0, 20])\n\nplt.tight_layout()\nplt.show()"
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "## Improved Denoising Model Architecture\n\nOur denoising model needs to predict the noise $\\epsilon_\\theta(x_t, t)$ that was added to create the noisy image $x_t$. The model takes two inputs:\n1. **Noisy image** $x_t$ at timestep $t$\n2. **Timestep** $t$ (embedded as a feature)\n\nThe key architectural components are:\n\n1. **Time Embedding**: We embed the timestep $t$ using sinusoidal positional encoding (similar to transformers)\n2. **U-Net-like Architecture**: We use a simplified U-Net with residual connections and attention\n3. **Residual Blocks**: Each block combines convolution, normalization, and time conditioning\n4. **Attention Mechanism**: Simple self-attention to capture long-range dependencies\n\nThe loss function is simply the Mean Squared Error between predicted and actual noise:\n$$\\mathcal{L} = \\mathbb{E}_{t,x_0,\\epsilon} [||\\epsilon - \\epsilon_\\theta(x_t, t)||^2]$$"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "class TimeEmbedding(nn.Module):\n    \"\"\"Sinusoidal time embedding similar to transformers\"\"\"\n    def __init__(self, embed_dim):\n        super().__init__()\n        self.embed_dim = embed_dim\n        \n    def forward(self, t):\n        device = t.device\n        half_dim = self.embed_dim // 2\n        embeddings = np.log(10000) / (half_dim - 1)\n        embeddings = torch.exp(torch.arange(half_dim, device=device) * -embeddings)\n        embeddings = t[:, None] * embeddings[None, :]\n        embeddings = torch.cat([torch.sin(embeddings), torch.cos(embeddings)], dim=-1)\n        return embeddings\n\nclass ResidualBlock(nn.Module):\n    \"\"\"Residual block with time conditioning\"\"\"\n    def __init__(self, in_channels, out_channels, time_embed_dim):\n        super().__init__()\n        self.conv1 = nn.Conv2d(in_channels, out_channels, 3, padding=1)\n        self.conv2 = nn.Conv2d(out_channels, out_channels, 3, padding=1)\n        self.time_mlp = nn.Sequential(\n            nn.Linear(time_embed_dim, out_channels),\n            nn.ReLU()\n        )\n        self.norm1 = nn.GroupNorm(8, out_channels)\n        self.norm2 = nn.GroupNorm(8, out_channels)\n        self.shortcut = nn.Conv2d(in_channels, out_channels, 1) if in_channels != out_channels else nn.Identity()\n        \n    def forward(self, x, time_emb):\n        # First convolution\n        h = self.conv1(x)\n        h = self.norm1(h)\n        h = F.relu(h)\n        \n        # Add time conditioning\n        time_cond = self.time_mlp(time_emb)\n        h = h + time_cond.unsqueeze(-1).unsqueeze(-1)\n        \n        # Second convolution\n        h = self.conv2(h)\n        h = self.norm2(h)\n        h = F.relu(h)\n        \n        # Skip connection\n        return h + self.shortcut(x)\n\nclass AttentionBlock(nn.Module):\n    \"\"\"Simple self-attention block\"\"\"\n    def __init__(self, channels):\n        super().__init__()\n        self.channels = channels\n        self.norm = nn.GroupNorm(8, channels)\n        self.qkv = nn.Conv2d(channels, channels * 3, 1)\n        self.proj = nn.Conv2d(channels, channels, 1)\n        \n    def forward(self, x):\n        b, c, h, w = x.shape\n        x_norm = self.norm(x)\n        qkv = self.qkv(x_norm)\n        q, k, v = qkv.chunk(3, dim=1)\n        \n        # Reshape for attention\n        q = q.view(b, c, h * w).transpose(1, 2)\n        k = k.view(b, c, h * w).transpose(1, 2)\n        v = v.view(b, c, h * w).transpose(1, 2)\n        \n        # Compute attention\n        attn = torch.softmax(q @ k.transpose(-2, -1) / (c ** 0.5), dim=-1)\n        out = attn @ v\n        \n        # Reshape back\n        out = out.transpose(1, 2).view(b, c, h, w)\n        out = self.proj(out)\n        \n        return x + out\n\nclass ImprovedDenoiser(nn.Module):\n    \"\"\"Improved U-Net-like denoiser with time conditioning and attention\"\"\"\n    def __init__(self, in_channels=1, model_channels=64, time_embed_dim=128):\n        super().__init__()\n        \n        self.time_embed = nn.Sequential(\n            TimeEmbedding(time_embed_dim),\n            nn.Linear(time_embed_dim, time_embed_dim),\n            nn.ReLU(),\n            nn.Linear(time_embed_dim, time_embed_dim)\n        )\n        \n        # Encoder\n        self.init_conv = nn.Conv2d(in_channels, model_channels, 3, padding=1)\n        \n        self.down1 = ResidualBlock(model_channels, model_channels, time_embed_dim)\n        self.down2 = ResidualBlock(model_channels, model_channels * 2, time_embed_dim)\n        self.down3 = ResidualBlock(model_channels * 2, model_channels * 4, time_embed_dim)\n        \n        # Middle with attention\n        self.mid = ResidualBlock(model_channels * 4, model_channels * 4, time_embed_dim)\n        self.mid_attn = AttentionBlock(model_channels * 4)\n        \n        # Decoder\n        self.up1 = ResidualBlock(model_channels * 4, model_channels * 2, time_embed_dim)\n        self.up2 = ResidualBlock(model_channels * 2, model_channels, time_embed_dim)\n        self.up3 = ResidualBlock(model_channels, model_channels, time_embed_dim)\n        \n        # Output\n        self.out_conv = nn.Conv2d(model_channels, in_channels, 3, padding=1)\n        \n    def forward(self, x, t):\n        # Time embedding\n        t_emb = self.time_embed(t.float())\n        \n        # Initial convolution\n        x = self.init_conv(x)\n        \n        # Encoder\n        x1 = self.down1(x, t_emb)\n        x2 = self.down2(x1, t_emb)\n        x3 = self.down3(x2, t_emb)\n        \n        # Middle\n        x = self.mid(x3, t_emb)\n        x = self.mid_attn(x)\n        \n        # Decoder (with skip connections)\n        x = self.up1(x + x3, t_emb)\n        x = self.up2(x + x2, t_emb)\n        x = self.up3(x + x1, t_emb)\n        \n        # Output\n        return self.out_conv(x)\n\n# Create the improved model\nmodel = ImprovedDenoiser()\ntotal_params = sum(p.numel() for p in model.parameters())\nprint(f\"Model created with {total_params:,} parameters\")\n\n# Test the model\ntest_input = torch.randn(2, 1, 8, 8)\ntest_time = torch.randint(0, 1000, (2,))\ntest_output = model(test_input, test_time)\nprint(f\"Test input shape: {test_input.shape}\")\nprint(f\"Test output shape: {test_output.shape}\")\nprint(f\"Model successfully processes input → output\")"
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "## Training the Denoising Model\n\nNow we train our model to predict the noise that was added to images. The training process is surprisingly simple:\n\n1. **Sample a batch** of clean images $x_0$\n2. **Sample random timesteps** $t$ for each image\n3. **Sample noise** $\\epsilon \\sim \\mathcal{N}(0, I)$\n4. **Create noisy images** $x_t = \\sqrt{\\bar{\\alpha}_t} x_0 + \\sqrt{1-\\bar{\\alpha}_t} \\epsilon$\n5. **Predict the noise** $\\epsilon_\\theta(x_t, t)$\n6. **Compute loss** $\\mathcal{L} = ||\\epsilon - \\epsilon_\\theta(x_t, t)||^2$\n\nThe key insight is that by training on all timesteps simultaneously, the model learns to denoise at any level of noise corruption."
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# Training setup\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nmodel = model.to(device)\nscheduler.betas = scheduler.betas.to(device)\nscheduler.alphas = scheduler.alphas.to(device)\nscheduler.alphas_cumprod = scheduler.alphas_cumprod.to(device)\n\noptimizer = optim.AdamW(model.parameters(), lr=2e-4, weight_decay=0.01)\ncriterion = nn.MSELoss()\n\nnum_epochs = 50\nlosses = []\nepoch_losses = []\n\nprint(f\"Training on device: {device}\")\nprint(f\"Training for {num_epochs} epochs with {len(dataloader)} batches per epoch\")\n\n# Training loop with progress tracking\nfor epoch in range(num_epochs):\n    model.train()\n    epoch_loss = 0\n    batch_losses = []\n    \n    pbar = tqdm(dataloader, desc=f'Epoch {epoch+1}/{num_epochs}')\n    for batch_idx, (x0,) in enumerate(pbar):\n        x0 = x0.to(device)\n        batch_size = x0.size(0)\n        \n        # Sample random timesteps\n        t = torch.randint(0, scheduler.num_timesteps, (batch_size,), device=device)\n        \n        # Generate noise\n        noise = torch.randn_like(x0)\n        \n        # Add noise to clean images\n        x_noisy = scheduler.add_noise(x0, t, noise)\n        \n        # Predict noise\n        predicted_noise = model(x_noisy, t)\n        \n        # Compute loss\n        loss = criterion(predicted_noise, noise)\n        \n        # Backpropagation\n        optimizer.zero_grad()\n        loss.backward()\n        \n        # Gradient clipping for stability\n        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n        \n        optimizer.step()\n        \n        # Track losses\n        batch_loss = loss.item()\n        epoch_loss += batch_loss\n        batch_losses.append(batch_loss)\n        \n        # Update progress bar\n        pbar.set_postfix({'loss': f'{batch_loss:.4f}'})\n    \n    avg_loss = epoch_loss / len(dataloader)\n    epoch_losses.append(avg_loss)\n    losses.extend(batch_losses)\n    \n    print(f'Epoch {epoch+1}/{num_epochs}, Average Loss: {avg_loss:.4f}')\n    \n    # Save some intermediate results every 10 epochs\n    if (epoch + 1) % 10 == 0:\n        model.eval()\n        with torch.no_grad():\n            # Test denoising on a sample\n            sample_x0 = X_tensor[0:1].to(device)\n            sample_t = torch.tensor([500], device=device)\n            sample_noise = torch.randn_like(sample_x0)\n            sample_noisy = scheduler.add_noise(sample_x0, sample_t, sample_noise)\n            sample_pred_noise = model(sample_noisy, sample_t)\n            \n            # Show denoising quality\n            print(f\"  Noise prediction MSE: {F.mse_loss(sample_pred_noise, sample_noise):.4f}\")\n\nprint(\"Training completed!\")\n\n# Plot training progress\nfig, axes = plt.subplots(1, 3, figsize=(18, 5))\n\n# Loss over batches\naxes[0].plot(losses, alpha=0.7)\naxes[0].set_title('Training Loss (per batch)')\naxes[0].set_xlabel('Batch')\naxes[0].set_ylabel('MSE Loss')\naxes[0].grid(True)\n\n# Loss over epochs\naxes[1].plot(epoch_losses, 'o-')\naxes[1].set_title('Training Loss (per epoch)')\naxes[1].set_xlabel('Epoch')\naxes[1].set_ylabel('Average MSE Loss')\naxes[1].grid(True)\n\n# Loss distribution\naxes[2].hist(losses, bins=50, alpha=0.7, edgecolor='black')\naxes[2].set_title('Distribution of Batch Losses')\naxes[2].set_xlabel('Loss')\naxes[2].set_ylabel('Frequency')\naxes[2].grid(True)\n\nplt.tight_layout()\nplt.show()"
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "## Reverse Diffusion: Sampling New Images\n\nNow comes the magic! To generate new images, we reverse the diffusion process. We start with pure noise and gradually denoise it using our trained model.\n\nThe reverse process follows this formula:\n$$x_{t-1} = \\frac{1}{\\sqrt{\\alpha_t}} \\left( x_t - \\frac{\\beta_t}{\\sqrt{1-\\bar{\\alpha}_t}} \\epsilon_\\theta(x_t, t) \\right) + \\sigma_t z$$\n\nwhere $z \\sim \\mathcal{N}(0, I)$ is additional noise (except for the final step).\n\nThis is essentially asking: \"If I know the noise that was added, how can I remove it to get the less noisy image?\""
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "def sample_images(model, scheduler, num_samples=8, device='cpu', show_progress=True):\n    \"\"\"Generate images using the trained diffusion model\"\"\"\n    model.eval()\n    \n    with torch.no_grad():\n        # Start with pure noise\n        x = torch.randn(num_samples, 1, 8, 8, device=device)\n        \n        # Store intermediate steps for visualization\n        intermediate_steps = []\n        save_steps = [999, 800, 600, 400, 200, 100, 50, 0]\n        \n        # Reverse diffusion process\n        iterator = reversed(range(scheduler.num_timesteps))\n        if show_progress:\n            iterator = tqdm(iterator, desc=\"Sampling\")\n            \n        for i, t in enumerate(iterator):\n            t_tensor = torch.full((num_samples,), t, device=device)\n            \n            # Predict noise\n            predicted_noise = model(x, t_tensor)\n            \n            # Remove predicted noise\n            alpha_t = scheduler.alphas[t]\n            alpha_cumprod_t = scheduler.alphas_cumprod[t]\n            beta_t = scheduler.betas[t]\n            \n            # Compute x_{t-1}\n            x = (1 / torch.sqrt(alpha_t)) * (x - (beta_t / torch.sqrt(1 - alpha_cumprod_t)) * predicted_noise)\n            \n            # Add noise if not the last step\n            if t > 0:\n                noise = torch.randn_like(x)\n                x = x + torch.sqrt(beta_t) * noise\n            \n            # Save intermediate steps\n            if t in save_steps:\n                intermediate_steps.append((t, x.clone().cpu()))\n    \n    return x.cpu(), intermediate_steps\n\n# Generate multiple batches of samples\nprint(\"Generating samples...\")\nall_samples = []\nall_intermediates = []\n\nfor i in range(5):  # Generate 5 batches of 8 samples each\n    samples, intermediates = sample_images(model, scheduler, num_samples=8, device=device, show_progress=(i==0))\n    all_samples.append(samples)\n    all_intermediates.append(intermediates)\n    print(f\"Generated batch {i+1}/5\")\n\n# Combine all samples\ngenerated_images = torch.cat(all_samples, dim=0)\nprint(f\"Generated {len(generated_images)} total images\")\n\n# Show multiple generations\nfig, axes = plt.subplots(5, 8, figsize=(16, 10))\nfig.suptitle('Multiple Generated Samples (5 batches × 8 samples)', fontsize=16)\n\nfor batch_idx in range(5):\n    for sample_idx in range(8):\n        img_idx = batch_idx * 8 + sample_idx\n        axes[batch_idx, sample_idx].imshow(generated_images[img_idx, 0], cmap='gray')\n        axes[batch_idx, sample_idx].axis('off')\n        axes[batch_idx, sample_idx].set_title(f'B{batch_idx+1}S{sample_idx+1}', fontsize=8)\n\nplt.tight_layout()\nplt.show()\n\n# Compare with original digits\nfig, axes = plt.subplots(2, 10, figsize=(20, 6))\nfig.suptitle('Original Digits vs Generated Samples', fontsize=16)\n\n# Original digits (one of each class)\nfor i in range(10):\n    digit_idx = np.where(digits.target == i)[0][0]\n    axes[0, i].imshow(X[digit_idx], cmap='gray')\n    axes[0, i].set_title(f'Original {i}')\n    axes[0, i].axis('off')\n\n# Generated samples\nfor i in range(10):\n    axes[1, i].imshow(generated_images[i, 0], cmap='gray')\n    axes[1, i].set_title(f'Generated {i}')\n    axes[1, i].axis('off')\n\nplt.tight_layout()\nplt.show()\n\n# Show quality metrics\nprint(\"\\nGenerated Image Statistics:\")\nprint(f\"Mean pixel value: {generated_images.mean():.3f}\")\nprint(f\"Std pixel value: {generated_images.std():.3f}\")\nprint(f\"Min pixel value: {generated_images.min():.3f}\")\nprint(f\"Max pixel value: {generated_images.max():.3f}\")\n\nprint(\"\\nOriginal Image Statistics:\")\nprint(f\"Mean pixel value: {X_tensor.mean():.3f}\")\nprint(f\"Std pixel value: {X_tensor.std():.3f}\")\nprint(f\"Min pixel value: {X_tensor.min():.3f}\")\nprint(f\"Max pixel value: {X_tensor.max():.3f}\")"
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "## Visualizing the Reverse Process\n\nLet's see how the denoising process works step by step. We'll track the intermediate states as we go from pure noise to clean digits."
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# Visualize the reverse diffusion process\nintermediates = all_intermediates[0]  # Use first batch's intermediates\n\nfig, axes = plt.subplots(len(intermediates), 8, figsize=(16, 20))\nfig.suptitle('Reverse Diffusion Process: From Noise to Digits', fontsize=16)\n\nfor step_idx, (timestep, images) in enumerate(intermediates):\n    for sample_idx in range(8):\n        axes[step_idx, sample_idx].imshow(images[sample_idx, 0], cmap='gray')\n        axes[step_idx, sample_idx].axis('off')\n        if sample_idx == 0:\n            axes[step_idx, sample_idx].set_ylabel(f't = {timestep}', fontsize=12)\n        if step_idx == 0:\n            axes[step_idx, sample_idx].set_title(f'Sample {sample_idx+1}', fontsize=10)\n\nplt.tight_layout()\nplt.show()\n\n# Show the progression for a single sample\nsample_idx = 0\nfig, axes = plt.subplots(1, len(intermediates), figsize=(20, 3))\nfig.suptitle(f'Single Sample Progression: From Noise to Digit', fontsize=16)\n\nfor step_idx, (timestep, images) in enumerate(intermediates):\n    axes[step_idx].imshow(images[sample_idx, 0], cmap='gray')\n    axes[step_idx].set_title(f't = {timestep}')\n    axes[step_idx].axis('off')\n\nplt.tight_layout()\nplt.show()\n\n# Create a simple animation frame sequence\ndef create_animation_frames():\n    \"\"\"Create frames for animation\"\"\"\n    frames = []\n    sample_idx = 0\n    \n    for timestep, images in intermediates:\n        frames.append(images[sample_idx, 0].numpy())\n    \n    return frames\n\n# Create and display animation frames\nanimation_frames = create_animation_frames()\n\n# Show animation frames in a grid\nfig, axes = plt.subplots(2, 4, figsize=(12, 6))\nfig.suptitle('Animation Frames: Reverse Diffusion Process', fontsize=16)\n\nfor i, frame in enumerate(animation_frames):\n    row = i // 4\n    col = i % 4\n    axes[row, col].imshow(frame, cmap='gray')\n    axes[row, col].set_title(f'Frame {i+1}')\n    axes[row, col].axis('off')\n\nplt.tight_layout()\nplt.show()\n\n# Show pixel intensity evolution\nfig, axes = plt.subplots(1, 2, figsize=(15, 5))\n\n# Plot mean pixel intensity over time\nmean_intensities = []\nstd_intensities = []\ntimesteps_list = []\n\nfor timestep, images in intermediates:\n    mean_intensities.append(images.mean().item())\n    std_intensities.append(images.std().item())\n    timesteps_list.append(timestep)\n\naxes[0].plot(timesteps_list, mean_intensities, 'o-', label='Mean Intensity')\naxes[0].fill_between(timesteps_list, \n                     np.array(mean_intensities) - np.array(std_intensities),\n                     np.array(mean_intensities) + np.array(std_intensities),\n                     alpha=0.3)\naxes[0].set_xlabel('Timestep')\naxes[0].set_ylabel('Pixel Intensity')\naxes[0].set_title('Pixel Intensity Evolution During Sampling')\naxes[0].legend()\naxes[0].grid(True)\n\n# Show histogram evolution\nsample_images = [images[0, 0].numpy().flatten() for _, images in intermediates]\ncolors = plt.cm.viridis(np.linspace(0, 1, len(sample_images)))\n\nfor i, (pixels, color) in enumerate(zip(sample_images, colors)):\n    axes[1].hist(pixels, bins=30, alpha=0.5, color=color, \n                label=f't={timesteps_list[i]}' if i % 2 == 0 else '')\n\naxes[1].set_xlabel('Pixel Value')\naxes[1].set_ylabel('Frequency')\naxes[1].set_title('Pixel Value Distribution Evolution')\naxes[1].legend()\naxes[1].grid(True)\n\nplt.tight_layout()\nplt.show()"
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "## Creating an Animated GIF\n\nLet's create an animated visualization of the diffusion process to really see how the model transforms noise into recognizable digits!"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# Create a detailed animation of the reverse diffusion process\ndef create_detailed_animation():\n    \"\"\"Create detailed animation frames showing the full reverse process\"\"\"\n    model.eval()\n    \n    with torch.no_grad():\n        # Start with pure noise (single sample for cleaner animation)\n        x = torch.randn(1, 1, 8, 8, device=device)\n        \n        # Store ALL intermediate steps\n        all_frames = []\n        \n        # Reverse diffusion process\n        for t in tqdm(reversed(range(scheduler.num_timesteps)), desc=\"Creating animation\"):\n            # Save every 50th frame to keep animation manageable\n            if t % 50 == 0 or t < 50:\n                all_frames.append((t, x[0, 0].cpu().numpy().copy()))\n            \n            t_tensor = torch.full((1,), t, device=device)\n            \n            # Predict noise\n            predicted_noise = model(x, t_tensor)\n            \n            # Remove predicted noise\n            alpha_t = scheduler.alphas[t]\n            alpha_cumprod_t = scheduler.alphas_cumprod[t]\n            beta_t = scheduler.betas[t]\n            \n            # Compute x_{t-1}\n            x = (1 / torch.sqrt(alpha_t)) * (x - (beta_t / torch.sqrt(1 - alpha_cumprod_t)) * predicted_noise)\n            \n            # Add noise if not the last step\n            if t > 0:\n                noise = torch.randn_like(x)\n                x = x + torch.sqrt(beta_t) * noise\n    \n    return all_frames\n\n# Create animation frames\nprint(\"Creating detailed animation frames...\")\nanimation_frames = create_detailed_animation()\n\n# Create the animation\nfig, ax = plt.subplots(figsize=(6, 6))\nax.set_xlim(0, 8)\nax.set_ylim(0, 8)\nax.set_aspect('equal')\n\ndef animate(frame_idx):\n    ax.clear()\n    timestep, image = animation_frames[frame_idx]\n    \n    # Display the image\n    im = ax.imshow(image, cmap='gray', vmin=-2, vmax=2)\n    ax.set_title(f'Diffusion Sampling: t = {timestep}', fontsize=14)\n    ax.axis('off')\n    \n    return [im]\n\n# Create animation\nanim = animation.FuncAnimation(fig, animate, frames=len(animation_frames), \n                             interval=200, blit=False, repeat=True)\n\n# Save as GIF\nprint(\"Saving animation as GIF...\")\nanim.save('diffusion_process.gif', writer='pillow', fps=5)\nprint(\"Animation saved as 'diffusion_process.gif'\")\n\n# Display the animation in notebook\nplt.show()\n\n# Show key frames from the animation\nkey_frames = animation_frames[::len(animation_frames)//8]  # Show 8 key frames\nfig, axes = plt.subplots(1, len(key_frames), figsize=(20, 3))\nfig.suptitle('Key Frames from Diffusion Animation', fontsize=16)\n\nfor i, (timestep, image) in enumerate(key_frames):\n    axes[i].imshow(image, cmap='gray')\n    axes[i].set_title(f't = {timestep}')\n    axes[i].axis('off')\n\nplt.tight_layout()\nplt.show()\n\n# Create a side-by-side comparison animation\ndef create_comparison_animation():\n    \"\"\"Create animation showing multiple samples side by side\"\"\"\n    model.eval()\n    \n    with torch.no_grad():\n        # Start with pure noise (4 samples)\n        x = torch.randn(4, 1, 8, 8, device=device)\n        \n        # Store frames every 25 steps\n        frames = []\n        \n        for t in tqdm(reversed(range(scheduler.num_timesteps)), desc=\"Creating comparison\"):\n            if t % 25 == 0 or t < 25:\n                frames.append((t, x.cpu().numpy().copy()))\n            \n            t_tensor = torch.full((4,), t, device=device)\n            predicted_noise = model(x, t_tensor)\n            \n            alpha_t = scheduler.alphas[t]\n            alpha_cumprod_t = scheduler.alphas_cumprod[t]\n            beta_t = scheduler.betas[t]\n            \n            x = (1 / torch.sqrt(alpha_t)) * (x - (beta_t / torch.sqrt(1 - alpha_cumprod_t)) * predicted_noise)\n            \n            if t > 0:\n                noise = torch.randn_like(x)\n                x = x + torch.sqrt(beta_t) * noise\n    \n    return frames\n\n# Create comparison frames\ncomparison_frames = create_comparison_animation()\n\n# Show final comparison\nfig, axes = plt.subplots(len(comparison_frames), 4, figsize=(12, 20))\nfig.suptitle('Multiple Samples Diffusion Process', fontsize=16)\n\nfor step_idx, (timestep, images) in enumerate(comparison_frames):\n    for sample_idx in range(4):\n        axes[step_idx, sample_idx].imshow(images[sample_idx, 0], cmap='gray')\n        axes[step_idx, sample_idx].axis('off')\n        if sample_idx == 0:\n            axes[step_idx, sample_idx].set_ylabel(f't = {timestep}', fontsize=10)\n        if step_idx == 0:\n            axes[step_idx, sample_idx].set_title(f'Sample {sample_idx+1}', fontsize=10)\n\nplt.tight_layout()\nplt.show()\n\nprint(f\"Created {len(animation_frames)} animation frames\")\nprint(f\"Created {len(comparison_frames)} comparison frames\")"
  },
  {
   "cell_type": "markdown",
   "source": "## Key Takeaways and Next Steps\n\n### What We've Learned\n\n1. **Diffusion models are conceptually simple**: Add noise gradually, then learn to reverse it\n2. **The reparameterization trick is key**: We can jump to any timestep directly during training\n3. **Time conditioning is crucial**: The model needs to know how much noise to remove\n4. **Architecture matters**: U-Net with attention works better than simple CNN\n5. **Quality emerges gradually**: The model progressively refines noise into structured images\n\n### Improvements You Could Try\n\n1. **Better architectures**: Full U-Net, transformer-based models\n2. **Advanced sampling**: DDIM, DPM-Solver for faster sampling\n3. **Conditioning**: Add class labels for controlled generation\n4. **Better schedules**: Cosine, learned schedules\n5. **Larger datasets**: CIFAR-10, CelebA, etc.\n\n### Applications\n\n- **Image generation**: Create new images from noise\n- **Image editing**: Inpainting, outpainting, style transfer\n- **Data augmentation**: Generate training data\n- **Interpolation**: Smooth transitions between images\n\nThis minimal implementation shows that diffusion models, while mathematically sophisticated, can be implemented and understood with relatively simple code. The key is understanding the forward and reverse processes and how to train a model to predict noise effectively.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABdEAAAEpCAYAAAB822ttAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8hTgPZAAAACXBIWXMAAA9hAAAPYQGoP6dpAAA/5ElEQVR4nO3dd3xUVeL+8WfSIRAwJEBoIRTpvamUEFmKCAIisqIuBERUBJdF+dogQQEVFhUsWOgoIgiKgoBKU6qwKB2klwDSAoRQk5zfH/5mZEguJJQzuPt5v1557XrvmfvcmdwD4ZmbMy5jjBEAAAAAAAAAAMjEz9cnAAAAAAAAAADArYoSHQAAAAAAAAAAB5ToAAAAAAAAAAA4oEQHAAAAAAAAAMABJToAAAAAAAAAAA4o0QEAAAAAAAAAcECJDgAAAAAAAACAA0p0AAAAAAAAAAAcUKIDAAAAAAAAAOCAEh0AAOAWtnv3brlcLnXp0uWmZzVu3FgulyvT9pMnT+rpp59WdHS0AgIC5HK5tHv3bknSgQMH9Mgjj6ho0aLy8/PL8vE3ks3XA/ZkZGSoWrVqatmy5U3NcbrGnYwfP14ul0vjx4/32u5yudS4ceMbe3J/AYsWLZLL5VJiYqKVvM6dOys6Olrnzp2zkgcAAOCEEh0AAPiMuxC90hdypkuXLl6vX0BAgG677TZVrFhRDz/8sL744gtduHAhR8d87rnn9N5776l69ep68cUXlZCQoPz583vyJk+erLi4OPXv318JCQk34Vndutyl4qVfISEhKlWqlLp37+55swFXNn78eK1bt+6K5ezOnTs9b9S8++679k7uL8D95kCuXLm0f//+LMeULFlSISEhls/s+vTv319JSUl66623fH0qAADgf1yAr08AAACgdOnSeuSRR3x9Gv9VunXrpmLFiskYo1OnTum3337TN998o8mTJ6tChQqaMmWKqlat6vWYiRMn6syZM5mO9e2336pcuXKaOXOm1/YLFy5o/vz5atasmT755JOb+nzcihYtqs2bNytfvnxW8rKrVq1aatWqlSTpxIkTWrRokUaPHq3p06fr559/VpkyZXx8hreu9PR0DRw4ULGxsapbt67juLFjx8oYI5fLpTFjxujpp5+2eJbeNm/erNy5c/ss38m5c+c0YMAAjR079qYcv27dutq8ebMiIiJuyvEvV6ZMGbVt21ZvvPGGevfurdDQUCu5AAAAl6NEBwAAPlemTBlrywP8r3jsscd0xx13eG1LSUnRgAED9Pbbb6tZs2Zas2aNihQp4tlfokSJLI914MABNWrUKNP2Q4cOKSMjQ4ULF76xJ38FgYGBKl++vLW87Kpdu7bXNWyMUefOnTVp0iQNHjxY48aN893J3eK+/fZb7d27VwMGDHAck56ervHjxysqKkp33323Pv30U61Zs0Y1a9a0eKZ/uhWvQemPNyQnTpyovn37qlKlSjf8+Llz57b+3B955BFNnz5dn332mR577DGr2QAAAG4s5wIAAP4yNm7cqI4dO6pgwYIKDg5WTEyM+vTpo+PHj2caW7JkSZUsWVInTpxQ7969Vbx4cQUEBGj8+PH65z//KZfLpV9//dXrMffee69cLlemombOnDlyuVx64403PNsWLlyorl27qly5csqTJ4/y5Mmj2rVr66OPPsry3N1rKCclJalLly4qXLiw/Pz8tGjRIkl/lIRvvPGGypQpo5CQEJUpU0avvfaaMjIyru9Fu0TevHn11ltvKT4+Xr///rsGDRrktf/y9aLdS8MYY7R48WLPciVdunRR48aNFR0dLUmaMGGCZ5+7SL7S2tPu41661ElGRoZGjx6tunXrKjw8XLlz51bJkiXVtm1b/fjjj55xV1oTfe/everWrZuKFi2qoKAgFStWTN26ddO+ffsyjXWfX1paml599VXFxMQoODhYt99+u95///3svqSOXC6XevbsKUlatWpVpnPfsmWL7r//fkVERHi9FmlpaXrrrbdUrVo15cqVS/ny5VNcXJxmz57tmPX111+refPmKlCggEJCQlSyZEk9+uij2rBhg9e4Cxcu6M0331TNmjUVGhqqvHnzqmHDhvr6668zHfPkyZMaMGCAKlasqDx58ihfvnwqX7684uPjvV7Pc+fOafjw4apWrZry5cunPHnyqHTp0nrooYe0fv36bL1W7nXH27dv7zhm3rx5SkpKUqdOnRQfHy9JGjNmjOP4JUuWKDY2VqGhoSpQoIA6duyY5XXgdvz4cT3xxBMqVKiQcufOrTp16ujLL790HJ/VmuiXXtfvv/++KlSooJCQEEVHR2vgwIFZzuUzZ86oX79+Kl68uEJCQlS5cmV9/PHH17z2+KBBg5Senq4XXngh24/JyTXndF7btm1TfHy8YmJiFBISooiICNWsWVN9+/bNdIyUlBQlJCSoUqVKypUrl/Lnz68WLVpoyZIlWZ5fy5YtFRoayhtRAADAp7gTHQAA/CUsW7ZMzZo10/nz5/XAAw+oZMmSWrFihd5++23Nnj1by5cvV4ECBbwec/78ed19991KSUlR69atFRQUpEKFCikuLk4jRozQwoULVb16dUl/lNjuEmfhwoVex3EX3XFxcZ5tb7zxhrZv36477rhD7dq104kTJzR37lz16NFDW7du1fDhwzM9h2PHjunOO+9UeHi4OnbsqAsXLigsLEyS9Pjjj2vs2LGKiYlRz549de7cOb355ptatmzZjXoJPfr3769x48Zp6tSpeu+99xzL7rZt26pkyZIaOHCgoqOjPcV19erVdeLECVWvXl0jRoxQtWrV1LZtW0m65g9bfOGFFzR06FCVLl1anTp1Ut68eZWUlKSffvpJCxYsyPJO+Ett27ZNDRo00OHDh9W6dWtVqlRJGzdu1NixYzVr1iwtXbo0yyVVHnroIa1cuVL33HOP/P39NXXqVPXs2VOBgYHq3r37NT2Xq3FfN5UqVVLnzp11/PhxBQUFyRijjh07asaMGbr99tvVs2dPpaamaurUqWrVqpVGjBih3r17ex2rX79+GjZsmMLDw9W2bVsVLFhQ+/bt0w8//KBatWqpcuXKkv6YCy1atNCiRYtUo0YNdevWTRcvXtTs2bPVpk0bvfPOO57lUYwxat68uVauXKn69eurRYsW8vPz0+7du/Xll1+qc+fOKl68uKQ/Pvhx6tSpqlq1quLj4xUcHKy9e/dq4cKFat68uapUqXLF18IYo0WLFql8+fKedfaz4i7M//GPf6hy5coqXry4Jk+erOHDh2da53v+/Pm655575Ofnp44dO6pIkSKaP3++6tevr9tuuy3Tsc+cOaPGjRtr/fr1uvPOOxUbG6t9+/apY8eOatas2ZW/mVl47rnntGjRIrVq1UrNmjXTV199pcTERF24cEGDBw/2jEtPT1erVq20cOFCVatWTZ06ddLx48fVt2/fa55HjRs31j333KNvvvlGS5YsUYMGDa44/lquucsdOHBAdevWVWpqqu6991517NhRp0+f1rZt2/TOO+94/Vl4/PhxNWrUSBs3blTDhg3VvHlznTx5UjNnzlRcXJymTZvm+bPELSgoSLVq1dKyZcuUmprKki4AAMA3DAAAgI/s2rXLSDKlS5c2CQkJmb6WL19ujDEmPT3dlC1b1kgyc+fO9TrGCy+8YCSZbt26eW2Pjo42kkyzZs3MmTNnvPYlJycbPz8/07p1a8+2lStXGkmmSZMmRpLZs2ePZ1+dOnVM3rx5TVpammfbzp07Mz2fixcvmqZNmxp/f3+vxxtjjCQjycTHx3sdxxhjFi5caCSZatWqmdOnT3u279+/30RERBhJpnPnzld6KT06d+5sJHleOyfFixc3ksyOHTs822JjY01WPx5KMrGxsZm2u79/WZ2b07EuPcddu3Z5toWHh5uiRYua1NRUr7EZGRnm2LFjV828++67jSTz4Ycfem3/8MMPPd/XrM6vXr165uTJk57tW7ZsMQEBAaZcuXJZnvvl3N+7Hj16ZDrvRx55xEgyXbp08Tp3SaZ///6ZjjVx4kTPa33+/HnP9n379pmCBQuawMBAr+tu9uzZRpKpUqWKOXr0qNexLl68aA4dOuT57xdffNFIMomJiSYjI8Oz/dSpU6Z27domKCjIJCUlGWOMWbdunZFk2rVrl+kcz507Z1JSUowxxpw4ccK4XC5Tu3btTNd0WlqaSU5OvuJrZ4wxGzduNJLMww8/7Djm8OHDJjAw0FSpUsWzzT3vP/nkE6+x6enpplSpUsblcpmffvrJsz0jI8N06tTJ8/pfKiEhwUgy3bt399o+b948z/hx48Z57ctqTriv65iYGHPgwAHP9iNHjpj8+fObvHnzen1fR48ebSSZ++67z6Snp3u2b9682YSEhBhJJiEhwfF1uZT7ej548KBZu3at8fPzM3fddZfXmOjoaBMcHOy1LafXnPt6v/S8Ro4caSSZESNGZDqvI0eOeP23+3swduxYr+2HDh0yxYsXN5GRkebs2bOZjtOnTx8jySxYsODqLwYAAMBNwHIuAADA53bs2KGBAwdm+lqxYoUkaenSpdq2bZvuueceNW/e3OuxL730kgoUKKDJkyfrwoULmY49bNgw5cqVy2tb/vz5Va1aNf34449KT0+X9Mfd55cuU7BgwQJJ0qlTp7RmzRo1bNhQ/v7+nmPExMRkygoICNATTzyh9PT0THezS3/cUTl06FCv40h/fKCnJA0YMMDrLsuiRYvqmWeeyfpFu07utdCPHj16U45/LYKCghQQ4P2Lki6XS+Hh4Vd83L59+7RgwQJVrFgx093j3bt3V4UKFTR//vwsl/N47bXXPL8NIEnlypVT/fr1tXXrVqWkpGT73FevXq3ExEQlJiaqT58+qlGjhj755BOFh4frpZde8hpbuHBhvfzyy5mOMX78eEnS0KFDFRQU5NlerFgx9enTRxcvXtSnn37q2f7ee+9JkkaMGJHptzACAgJUqFAhSX8slTNq1CiVKVNGAwYM8PrNg7x582rAgAG6cOGCZsyY4XWMy+eNJAUHBytPnjyS5FnqJzg4ONM17e/vf8U7y932798vSZ5zzcrEiRN18eJF/eMf//Bsc///y5d0WbJkiXbu3KlWrVp53YXtcrk0ZMiQTOfpPn5QUJBeeeUVr+3NmjVTkyZNrvocLte/f39FRUV5/jsiIkJt2rRRSkqKtm7d6tnu/jDeV199VX5+f/6zrHz58urcuXOOc92qVq2qTp06admyZfrqq6+uODan19yVZHW9XPoBpEePHtXnn3+uJk2aeJbkcStUqJCee+45HTlyRD/88EOm47ivD/f1AgAAYBvLuQAAAJ9r3ry55s6d67j/l19+kZT1UiGhoaGqXbu25s2bp99++82zfIUkhYSEOC4nERcXp19++UVr1qxRnTp1PEsqNGjQQIULF9bChQvVpUsXT9F+6VIu0h/r+v773//WV199pR07dig1NdVr/4EDBzJlxsTEeJVKbmvXrpUkNWzYMNO+rLbdCMaYm3Lca/Xggw/qgw8+UOXKldWxY0fFxsbqzjvvzNbSDe7rIzY2NtPSNC6XS40aNdLmzZu1du1azzIkbll9MGWxYsUkSSdOnFDevHmzdf7/+c9/9J///EfSH28GFC1aVN27d9dLL73kWTverVq1al6F5aXPI1euXKpbt26mfe5r/9J1/H/++WcFBwcrNjb2iue2detWJScnq0iRIho4cGCm/UeOHJEkbdmyRZJUoUIFValSRZMnT9a+ffvUtm1bNWzYUDVr1vQqocPCwtSiRQvNnTtXNWvW1AMPPKCGDRuqXr16WT6/rBw7dkySslxmxW3s2LHy8/NTp06dPNvKly+vOnXqaNGiRdq5c6dKlSol6cpzKTo6WsWLF/daiz8lJUW7du1SxYoVs/yA3IYNG2r+/PnZei5uV7um3NauXavQ0FBVrVo10/i77rpLH374YY5yLzVo0CBNmzZNL774olq3bp3lmwdSzq+5rLRq1UrPP/+8evbsqe+//14tWrRQgwYNdPvtt3uNW7VqldLT03Xu3Lks13rftm2bpD+uw1atWnntc7+Rdiu96QcAAP63UKIDAIBb3qlTpyQ5363qLr9Onjzptb1gwYKO633HxcXpzTff1MKFC1WjRg0tXbrUcxdz48aNPXeSu//30hL9woULaty4sdasWaMaNWro0UcfVYECBRQQEKDdu3drwoQJOn/+fKZMp/M/efKk/Pz8sizYr3SH7vU4ePCgJCkyMvKmHD+nRo4cqVKlSmn8+PEaNGiQBg0apJCQED344IMaPnx4lq+N27VeH5KUL1++TNvcd8O7f0shO3r06KEPPvggW2OdzvPUqVOZSn63rJ7DiRMnVLRoUa+7mLPi/uDdjRs3auPGjY7j3G8EBQQEaMGCBUpMTNSMGTM8Hw4ZERGhXr166aWXXvKUsl988YWGDBmizz77zHPHfd68edW1a1cNGTJEuXPnvuK5ue9ePnv2bJb7V6xYoU2bNqlp06ae355w69y5s1atWqVx48bp1VdflfTn61OwYMEsj1eoUCGvEj0743Mqu9fUlb7f1zvvo6Oj9eSTT+rtt9/WuHHjMn1YcnbO4Urz5lIxMTFavny5Bg4cqDlz5mjatGmS/vitjldffVUdOnSQ9Od1uHTpUi1dutTxeJe/ISn9eX1c7XoCAAC4WVjOBQAA3PLcy238/vvvWe53b790WQ5JjgW6JDVq1Ej+/v5auHChVq1apdOnT3uK8ri4OO3bt087duzQokWLlC9fPtWoUcPz2JkzZ2rNmjV67LHHtGbNGo0aNUqDBg1SYmKiWrRo4ZjpdD758uVTRkZGlndZOj3n67Fz507t27dPERERKlmy5A0/viRPsZuWlpZpX1alXGBgoJ577jlt3LhRSUlJmjx5sho2bKiJEyfq4YcfvmLWtV4fvuJ0HYSFheXoOeTPn1+HDh1SRkbGFfPcj2nfvr2MMY5f48aN8zwmIiJC7777rpKSkrRp0ya9++67KlCggBISEjR06FDPuNDQUA0ePFg7d+7Uzp07NWbMGJUvX14jRoxQnz59rvpauN/EcResl3Mv1/L999/L5XJ5fbk/CHX8+PGe18BdYB8+fDjL413++rpfm+yOv5HCwsI8vwVwM3JffvllhYWFKTEx0fFNipxec06qVq2q6dOn6/jx41q+fLkGDBig33//XR07dvQU5u7j9O3b94rXYUJCQqbju6+PW+VNPwAA8L+HEh0AANzy3AX2okWLMu07c+aMVq9erVy5cqlcuXLZPmZYWJhq1KihJUuW6LvvvpO/v78aNWokSbr77rslSTNmzNCvv/6qRo0aed3tu2PHDknSfffdl+m4P/30U7bPwa1atWqOj72W412N+67dv//971d8o+F6uJfnSEpK8tqekZHhWXLDSZEiRfTQQw9p7ty5Klu2rH744QfHElCSqlevLkn68ccfMy1TY4zxvIbucbeqGjVq6OzZs/r5558z7Vu8eLEk7+dQt25dnT9/3rPPSYUKFRQWFqbVq1fr4sWLOTonl8ulChUqeJbqkKSvv/46y7ExMTHq2rWrFi9erDx58jiOu1SlSpXk5+fnWcrjUqmpqfr888+VO3dudevWLcuvSpUqaf/+/Zo3b56kK8+lPXv2ZFoXPywsTDExMdq+fbsOHTqU6TE3Y/65VatWTampqVq3bl2mfcuWLbvu4xcoUED9+vVTUlKSRo4cmeWYnF5zVxMYGKg77rhDAwcO1MiRI2WM0axZsyRJderUkcvl0vLly3P8XNxryTstzwUAAHCzUaIDAIBbXv369VW6dGnNmTMn04fOvfbaazp69KgeeuihbK/D7BYXF6fTp0/rvffeU82aNT13sZYpU0bFihXTsGHDlJGRkWk9dPca10uWLPHavnjxYn388cc5fXqeD0l85ZVXvJYySEpK0ogRI3J8PCcpKSn617/+pfHjxysqKkovvvjiDTv25WrXri3pzw8udHvzzTe1a9cur23nz5/XggULMhXgqampSklJUWBgoOOazpJUokQJxcXFaePGjRo7dqzXvrFjx2rjxo26++67HZetuFW4P0zyhRde8Cq7k5KS9OabbyogIMDrrvyePXtKkp555plMd3KnpaV57iQOCAjQk08+qT179ujZZ5/NskjfsGGD527sXbt2adOmTZnGuI/nXoLlyJEjWZavycnJOn/+fJYfNHm5/Pnzq2rVqlq9enWm7//UqVOVkpKiDh06aPTo0Vl+DRkyRNKfd6w3aNBAMTExmjVrltf8NMboxRdfzHKJnkcffVQXLlzQgAEDvLZ/9913OV4PPSfc38v+/ft7/TbBli1bNGHChBuS0adPH0VFRen111/X6dOnM+3P6TWXlVWrVmV5J//l10vhwoX14IMPatmyZRo2bFiWn8uwcuVKnTlzJsvtUVFRKlu27BXPBQAA4GZhTXQAAHDL8/Pz0/jx49W8eXO1bNlSHTp0UHR0tFauXKkFCxaodOnSev3113N83Li4OA0bNkxHjhxRfHx8pn2TJk3y/P9LtW7dWiVLltTQoUO1YcMGVa5cWVu3btWsWbPUtm1bTZ8+PUfn0bhxY8XHx2vcuHGqUqWK2rVrp/Pnz+vzzz/XHXfc4bmTMydGjx6tuXPnyhijlJQU/fbbb/rxxx+VkpKiSpUqacqUKYqKisrxcbMrPj5eQ4cOVWJion799VeVLl1aq1ev1oYNGxQbG+t19/TZs2fVpEkTlSpVSvXq1VOJEiV0+vRpzZo1S4cOHdL//d//XfUNklGjRqlBgwbq3r27vvnmG1WsWFGbNm3S119/rcjISI0aNeqmPdcb5dFHH9WMGTM0c+ZMVa1aVa1atVJqaqqmTp2qY8eOafjw4Z4P0JSkli1b6tlnn9W///1vlS1bVu3atVPBggWVlJSk+fPn69lnn9U///lPSdLAgQO1Zs0ajRw5UrNnz1ZsbKwiIyOVlJSk9evXa+3atVq+fLkKFiyotWvXql27dqpTp44qV66swoULKykpSV999ZX8/f09a6QnJSWpXr16qlSpkmrWrKmiRYvq2LFjmjlzpi5evKh+/fpl63m3bdtWiYmJWrVqldcHXLqL8a5duzo+tmXLlipUqJC+/vprHTlyRJGRkfroo4/UsmVL/e1vf1PHjh1VpEgRLViwQAcPHlTVqlUz3fndr18/zZgxQx9//LE2btyoRo0aad++fZo6daruvfdezZ49O1vPI6fi4+M1adIkff3116pVq5aaN2+u48ePa8qUKWratKm++eabq653fzW5c+dWQkKCnnjiCUlScHCw1/6cXnNZ+fTTT/X++++rcePGKlOmjMLCwrRp0yZ9++23ioiI8Pr+vf/++9q6dav69eunSZMm6c4771S+fPm0b98+/ec//9G2bdt08OBBr7XPd+zYoV27dunJJ5+8rtcCAADguhgAAAAf2bVrl5Fkmjdvnq3x69atMw888ICJiIgwgYGBJjo62vTu3dscOXIk09jo6GgTHR19xeOlpKSYgIAAI8nMmTPHa9/YsWONJHPbbbeZ9PT0TI/duXOnad++vYmMjDS5c+c2derUMVOmTDELFy40kkxCQoLXeEkmNjbW8VzS0tLMa6+9ZkqVKmWCgoJMqVKlzJAhQ8z27duNJNO5c+crPhe3zp07G0meL39/f5M/f35TsWJF8/DDD5tp06aZCxcuZPnY2NhYk9WPh07n7v7+OZ3bmjVrTJMmTUzu3LlNWFiYadOmjdm2bZvnHHft2mWMMebChQvmjTfeMM2aNTPFihUzQUFBplChQiY2NtZMmTIl25m7d+828fHxJioqygQEBJioqCgTHx9vdu/ene3naozJdH5X4v5+9+jR46pjr/Z6GWPMxYsXzb///W9TpUoVExwcbPLmzWtiY2PNzJkzHR8zffp0ExcXZ/Lly2eCg4NNyZIlzaOPPmo2bNjgNS4tLc18+OGHpn79+iYsLMwEBwebEiVKmBYtWphRo0aZ06dPG2OM2bdvn3n++efNHXfcYQoWLGiCgoJMiRIlzAMPPGBWrlzpOV5ycrJJTEw0jRo1MlFRUSYoKMgUKVLEtGjRwsybN++qr4fb/v37jb+/v+nVq5dn25YtW4wkU7p06as+vm/fvkaSGT58uGfbjz/+aBo1amRy5cplwsPDTYcOHcyePXscv+/Hjh0zjz/+uImMjDQhISGmVq1aZsaMGWbcuHFGkhk3bpzX+KzmxJWum4SEBCPJLFy40Gv76dOnTd++fU2RIkVMcHCwqVixovnoo4/MF198YSSZt95666rP35g/r+eDBw9m2nfx4kVTrlw5I8kEBwdnuT+711xWf76tWLHC9OjRw1SuXNnkz5/f5MqVy5QtW9b07t3b7N27N9Mxzpw5Y4YOHWpq1aplQkNDTa5cuUxMTIxp27atmThxorl48aLX+MTERCPJ/Prrr9l6LQAAAG4GlzFZ/B4dAAAAAFjSqVMnfffdd9qzZ49CQ0N9fTo+9/LLL2vw4MH69ttvdc899/j6dHwmLS1Nt99+u0qWLKkFCxb4+nQAAMD/MNZEBwAAAOBTgwcP9nw+wf+SgwcPZtq2adMmjRw5Uvnz51dsbKwPzurWMWnSJO3evVvDhg3z9akAAID/cayJDgAAAMCnYmJiNGHCBB09etTXp2LVk08+qd27d6tu3bq67bbbtGPHDn3zzTe6ePGixowZ47U2+P8il8uljz/+WLVq1fL1qQAAgP9xLOcCAAAAAD7w6aef6oMPPtDmzZt18uRJ5cmTR3Xq1FHfvn3VvHlzX58eAAAA/j9KdAAAAAAAAAAAHLAmOgAAAAAAAAAADijRAQAAAAAAAABwQIkOAAAAAAAAAIADSnQAAAAAAAAAABxQov9FLFu2TImJiTpx4oRP8k+fPq1//vOfKlKkiEJCQlS9enVNmTLFJ+cC/BX4cs6mpKSoX79+atasmSIjI+VyuZSYmOg4fs2aNfrb3/6mPHnyKH/+/Lr//vu1c+fOLMe+8847Kl++vIKDgxUTE6OBAwfq4sWLN+mZALcmX87vX3/9Vffee69KlCihXLlyKTw8XHfeeac++eSTLMczv4Hs8+XcXrRokVwuV5ZfK1asyDSeuQ3kjC/nd5cuXRznd1ZznPkNZJ+vu7Kff/5ZzZs3V968eZUnTx7FxcVp6dKlmcYZYzRy5EjPfI2KitKTTz6p5OTkTGMPHjyoLl26qGDBggoJCVHVqlU1ZswYG08HV0GJ/hexbNkyDRw40Gd/MNx///2aMGGCEhISNGfOHNWpU0cPPfSQJk+e7JPzAW51vpyzx44d00cffaTz58+rbdu2Vxy7ZcsWNW7cWBcuXNDUqVM1duxY/fbbb2rYsKGOHDniNXbw4MF65plndP/992vevHl66qmnNGTIEPXs2fMmPhvg1uPL+X3ixAkVL15cQ4YM0bfffquJEyeqZMmSevTRRzVo0CCvscxvIGd8/fO2JA0ZMkTLly/3+qpcubLXGOY2kHO+nN/9+/fPNK+XL1+uiIgIFS1aVHXq1PGMZX4DOePLub1q1So1atRIZ8+e1aRJkzRp0iSdO3dOTZo00fLly73GPvvss+rTp4/atGmjWbNm6fnnn9fkyZPVtGlTrze+Tp48qQYNGmj+/PkaOnSoZs6cqZo1a+qxxx7Tm2++afsp4nIGfwnDhg0zksyuXbusZ8+ePdtIMpMnT/ba3rRpU1OkSBGTlpZm/ZyAW50v52xGRobJyMgwxhhz5MgRI8kkJCRkObZDhw4mIiLCnDx50rNt9+7dJjAw0PTr18+z7ejRoyYkJMQ8/vjjXo8fPHiwcblcZuPGjTf+iQC3KF/Obyf16tUzxYsX99rG/AZyxpdze+HChUaSmTZt2lXHMreBnLvV/u5etGiRkWRefvllr+3MbyBnfDm3mzdvbgoVKmRSU1M9206dOmUiIiLMXXfd5dm2f/9+4+/vb3r16uX1+MmTJxtJ5qOPPvJse+2114wks3r1aq+xzZo1M6GhoSY5OfnmPBlkC3ei/wUkJibqueeekyTFxMR4fu1r0aJFVvK//PJL5cmTRx06dPDaHh8frwMHDmjlypVWzgP4q/D1nHXnXU1aWppmzZql9u3bKywszLM9OjpacXFx+vLLLz3b5s6dq3Pnzik+Pt7rGPHx8TLG6Kuvvrph5w/cynw9v51EREQoICDA89/MbyBnbtW5fTnmNpBzt+L8HjNmjFwul7p27erZxvwGcsbXc3vp0qVq3LixcufO7dmWN29eNWrUSMuWLdPBgwclSStWrFB6erpatmzp9fhWrVpJkqZPn+51zEKFCqlWrVqZxqampmru3Lk36+kgGwKuPgS+9thjj+n48eN65513NGPGDEVFRUmSKlas6PgYY4zS09OzdfxL/9GdlQ0bNqhChQqZxlWtWtWz/6677spWFvC/wNdzNrt27Nihs2fPeubypapWrarvv/9e586dU0hIiDZs2CBJqlKlite4qKgoRUREePYD/+1ulfmdkZGhjIwMJScna9q0aZo3b57effddz37mN5Azt8rc7tmzp/7+978rd+7cuvPOO9W/f381aNDAs5+5DeTcrTK/3U6ePKkvvvhCTZo0UUxMjGc78xvIGV/P7QsXLig4ODjTdve29evXKyoqShcuXPDa7hYYGCiXy6V169Zl+5jr1q3T3//+92ydP248SvS/gGLFiqlEiRKSpBo1aqhkyZJXfcyECRMyvSvtxBhzxf3Hjh1TqVKlMm0PDw/37AfwJ1/P2exyz133XL5UeHi4jDFKTk5WVFSUjh07puDgYIWGhmY5lj8H8L/iVpnfTz31lD788ENJUlBQkEaOHKkePXp49jO/gZzx9dzOly+fnnnmGTVu3FgFChTQ9u3bNWzYMDVu3FizZ89W8+bNJTG3gWvh6/l9uc8++0xnz55Vt27dvLYzv4Gc8fXcrlixolasWKGMjAz5+f2x0EdaWppntQb3PHSX+kuXLlVcXJzn8cuWLZMxxmu+VqxYUT/88IP27t3reW6StGTJEq9jwjco0f9LtW7dWqtWrbphx7vS0hDZWTYCwJXd6DmbE9md3/w5AFybmzG/X3zxRT322GM6fPiwvvnmGz399NNKTU3Vs88+6zWO+Q3cPDdybteoUUM1atTw/HfDhg3Vrl07ValSRf369fOU6G7MbeDmupk/m48ZM0YFChRQu3btstzP/AZunhs5t3v16qVu3brp6aef1ksvvaSMjAwNHDhQe/bskSRPsV6tWjU1atRIw4YNU7ly5dS0aVNt2rRJTzzxhPz9/T3jJOnxxx/XqFGj9PDDD+uDDz5Q4cKFNWXKFH3++edex4RvUKL/lwoPD1e+fPluyLEKFCiQ5btdx48f92QBuD43cs5mV4ECBSRl/W728ePH5XK5lD9/fs/Yc+fO6cyZM15rvrnHXr5mG4A/3Yz5XaJECc/dKe71FV944QV17txZkZGRzG/Agpv9d3f+/PnVqlUrffDBBzp79qxy5crF3AYsuVnze926dVq9erWeeeaZTEs2ML+Bm+9Gzu2uXbvqyJEjGjRokEaNGiVJuvPOO/Xss8/qjTfeUNGiRT1jp02bpi5duujBBx+U9Mdvkvbp00c//PCDTpw44RlXoUIFffnll+rRo4cqV64sSSpevLiGDx+uXr16eR0T9vEWxn+pCRMmKDAwMFtfV1OlShVt3rxZaWlpXtvXr18vSZ6JDeDa3cg5m12lS5dWrly5PHP5UuvXr1eZMmUUEhIi6c/1Fi8fe+jQIR09epQ/B4ArsDG/69atq7S0NO3cuVMS8xuwwcbcdv8qufuuUuY2YMfNmt9jxoyR9MdazpdjfgM3342e2//3f/+no0ePav369dq9e7eWLVum5ORkhYaGer2ZVbBgQX377bf6/ffftXbtWh0+fFivvPKKfvvtNzVq1MjrmPfcc4/27Nmj3377TZs2bdKuXbs8b7JdPhZ2cSf6X4T7XeqzZ89ma/yN/BWVdu3a6eOPP9b06dPVsWNHz/YJEyaoSJEiqlev3g3JAf6b+HLOZldAQIBat26tGTNmaOjQocqbN68kae/evVq4cKH69OnjGduiRQuFhIRo/PjxXnN+/Pjxcrlcatu2rdVzB3zpVpzfCxculJ+fn+czTJjfQM7danM7OTlZs2bNUvXq1T3FGXMbuDa3wvw+f/68PvnkE9WtWzfLkpv5DeTcrTC3g4ODPXN67969+vzzz9W9e3flypUr09iCBQuqYMGCkqSRI0cqNTVVTz/9dKZxLpdLZcuWlfTHh42OGDFC1atXp0T3MUr0vwj3O80jRoxQ586dFRgYqHLlynn+Yr1cgQIFPO9UXa977rlHTZs21ZNPPqlTp06pTJky+uyzzzR37lx98skn8vf3vyE5wH8TX85ZSZozZ45SU1OVkpIiSdq0aZO++OILSX8s/eD+tc+BAweqTp06atWqlZ5//nmdO3dOAwYMUEREhPr27es5Xnh4uF5++WX1799f4eHhatasmVatWqXExEQ99thjV/wEdOC/jS/n9+OPP66wsDDVrVtXhQoV0tGjRzVt2jR9/vnneu655xQZGekZy/wGcsaXc7tTp04qUaKEateurYiICG3btk3Dhw/X77//rvHjx3uNZW4DOefrn80l6auvvtLx48ezvAvdjfkN5Iwv5/aGDRs0ffp01a5dW8HBwVq7dq1ef/11lS1bVq+++qrX2I8//ljSH79xcuLECc2ZM0djxozRkCFDVLNmTa+xvXr18nzQ+M6dOzVy5Ejt379fixcvviHnjetg8JfxwgsvmCJFihg/Pz8jySxcuNBadkpKiundu7cpXLiwCQoKMlWrVjWfffaZtXzgr8iXczY6OtpIyvJr165dXmNXr15tmjRpYnLnzm3CwsJM27Ztzfbt27M87ogRI8ztt99ugoKCTIkSJUxCQoK5cOGChWcE3Fp8Nb/Hjh1rGjZsaCIiIkxAQIDJnz+/iY2NNZMmTcpyPPMbyBlfze3XXnvNVK9e3eTLl8/4+/ubyMhI065dO/Pzzz9nOZ65DeScL382N8aYpk2bmtDQUHPq1KkrjmN+Aznjq7m9detW06hRIxMeHm6CgoJMmTJlzMsvv2xOnz6daeyHH35oKlSoYHLnzm3y5MljGjZsaL766qssj9umTRsTFRVlAgMDTeHChU2XLl3M7t27b/bTQTa4jPn/C+0BAAAAAAAAAAAvfLAoAAAAAAAAAAAOKNEBAAAAAAAAAHBAiQ4AAAAAAAAAgANKdAAAAAAAAAAAHFCiAwAAAAAAAADggBIdAAAAAAAAAAAHlOgAAAAAAAAAADgI8PUJAAAAAMiZN99803pmenq69cwCBQpYz1y+fLn1zGLFilnPlKR8+fJZz6xYsaL1zC1btljP7N27t/VMAABw82S7RA8NDb2Z55GltLQ065nt27e3nvn6669bz5w3b571TEl6+eWXrWeeOHHCemZGRob1zIsXL17zY7t27XoDzyR78uTJYz1z/fr11jMXLFhgPXPx4sXWMyVp9OjR1jPnzp1rPXPYsGHWM+Pj46/pcU899dQNPpOrO3TokPXMIkWKWM8MCLB/H8Lbb79tPVOS3nrrLeuZvvg55bbbbrOe+dlnn1nPBHBre/HFF61n1q1b13rm+fPnrWfWqVPHema/fv2sZ0rSHXfcYT3zu+++s57ZokUL65n/+te/rvmxw4cPv4Fnkj0///yz9Uxf/GxesGBB65lt2rSxnilJDz/8sPXMESNGWM/0xb9BvvzyyyvuZzkXAAAAAAAAAAAcUKIDAAAAAAAAAOCAEh0AAAAAAAAAAAeU6AAAAAAAAAAAOKBEBwAAAAAAAADAASU6AAAAAAAAAAAOKNEBAAAAAAAAAHBAiQ4AAAAAAAAAgANKdAAAAAAAAAAAHFCiAwAAAAAAAADggBIdAAAAAAAAAAAHlOgAAAAAAAAAADigRAcAAAAAAAAAwAElOgAAAAAAAAAADijRAQAAAAAAAABwQIkOAAAAAAAAAIADSnQAAAAAAAAAABxQogMAAAAAAAAA4IASHQAAAAAAAAAAB5ToAAAAAAAAAAA4oEQHAAAAAAAAAMABJToAAAAAAAAAAA4o0QEAAAAAAAAAcECJDgAAAAAAAACAA0p0AAAAAAAAAAAcUKIDAAAAAAAAAOCAEh0AAAAAAAAAAAcB2R147ty5m3keWQoODraemZCQYD2zRIkS1jPDw8OtZ0rSoUOHrGc+9NBD1jOnTp1qPfN63H777dYzp0+fbj1z4sSJ1jMHDhxoPbN27drWMyXp9ddft5558eJF65ldu3a1nhkfH39Nj/Pzs/9eeYcOHaxnHjx40Hpm3bp1rWc+9dRT1jMlqV69etYz33//feuZw4YNs555PfLmzWs9s2TJktYzt2zZYj0zf/781jO3bdtmPVOSqlWrZj2zf//+1jN79eplPfNa+eLv0QceeMB6Zrdu3axnZmRkWM+MjIy0nilJn376qfVMX3RG58+ft555PR555BHrmU2bNrWe6Yuf4/r06WM987XXXrOeKfnm3wS++Lupffv21jOvhjvRAQAAAAAAAABwQIkOAAAAAAAAAIADSnQAAAAAAAAAABxQogMAAAAAAAAA4IASHQAAAAAAAAAAB5ToAAAAAAAAAAA4oEQHAAAAAAAAAMABJToAAAAAAAAAAA4o0QEAAAAAAAAAcECJDgAAAAAAAACAA0p0AAAAAAAAAAAcUKIDAAAAAAAAAOCAEh0AAAAAAAAAAAeU6AAAAAAAAAAAOKBEBwAAAAAAAADAASU6AAAAAAAAAAAOKNEBAAAAAAAAAHBAiQ4AAAAAAAAAgANKdAAAAAAAAAAAHFCiAwAAAAAAAADggBIdAAAAAAAAAAAHlOgAAAAAAAAAADigRAcAAAAAAAAAwAElOgAAAAAAAAAADijRAQAAAAAAAABwQIkOAAAAAAAAAIADSnQAAAAAAAAAABxQogMAAAAAAAAA4CAguwODgoJu5nlkqVKlStYzb7/9duuZZcqUsZ65Z88e65mSNG/ePOuZNWrUsJ45Y8YM65nXY8mSJdYzX3/9deuZBw8etJ557tw565ljxoyxnilJHTp0sJ5ZvXp165nFixe3nnmtNmzYYD2ze/fu1jO/++4765m//fab9cyNGzdaz5SkX375xXrm6NGjrWfGx8dbz7wec+bMsZ4ZFxdnPdMXf3fXrFnTemZycrL1TElasWKF9cxWrVpZz9yyZYv1zGsVGBhoPbNNmzbWMx999FHrmQcOHLCeefjwYeuZkpSYmGg986OPPrKeWbBgQeuZ1+O+++6znlm/fn3rmb74N/Arr7xiPTM1NdV6piStXLnSeuawYcOsZ/qiE74a7kQHAAAAAAAAAMABJToAAAAAAAAAAA4o0QEAAAAAAAAAcECJDgAAAAAAAACAA0p0AAAAAAAAAAAcUKIDAAAAAAAAAOCAEh0AAAAAAAAAAAeU6AAAAAAAAAAAOKBEBwAAAAAAAADAASU6AAAAAAAAAAAOKNEBAAAAAAAAAHBAiQ4AAAAAAAAAgANKdAAAAAAAAAAAHFCiAwAAAAAAAADggBIdAAAAAAAAAAAHlOgAAAAAAAAAADigRAcAAAAAAAAAwAElOgAAAAAAAAAADijRAQAAAAAAAABwQIkOAAAAAAAAAIADSnQAAAAAAAAAABxQogMAAAAAAAAA4IASHQAAAAAAAAAAB5ToAAAAAAAAAAA4oEQHAAAAAAAAAMABJToAAAAAAAAAAA4o0QEAAAAAAAAAcECJDgAAAAAAAACAg4DsDjx37tzNPI8sRUVFWc9cu3at9czdu3dbz/Tz8837J2vWrLGe6XK5rGf66vW9VpGRkdYzjxw5Yj3TF9dCnTp1rGcmJydbz5SkJk2aWM/8+OOPrWcuWrTIeua1Wrx4sfXMwYMHW8985JFHrGf64u+zsmXLWs+UpNTUVOuZd999t/XMZ555xnpm165dr/mxX3755Q08k+xp2LCh9cxWrVpZz/ziiy+sZ1atWtV6piRVqlTJembFihWtZ44ePdp65rUaOnSo9cwHH3zQeubMmTOtZ5YrV856ZkxMjPVMSfr++++tZ1arVs165n333Wc983rUq1fPembr1q2tZ8bFxVnPfOWVV6xntmjRwnqmJH377bfWM6tXr249c8WKFdYza9WqdcX9f62mDwAAAAAAAAAAiyjRAQAAAAAAAABwQIkOAAAAAAAAAIADSnQAAAAAAAAAABxQogMAAAAAAAAA4IASHQAAAAAAAAAAB5ToAAAAAAAAAAA4oEQHAAAAAAAAAMABJToAAAAAAAAAAA4o0QEAAAAAAAAAcECJDgAAAAAAAACAA0p0AAAAAAAAAAAcUKIDAAAAAAAAAOCAEh0AAAAAAAAAAAeU6AAAAAAAAAAAOKBEBwAAAAAAAADAASU6AAAAAAAAAAAOKNEBAAAAAAAAAHBAiQ4AAAAAAAAAgANKdAAAAAAAAAAAHFCiAwAAAAAAAADggBIdAAAAAAAAAAAHlOgAAAAAAAAAADigRAcAAAAAAAAAwAElOgAAAAAAAAAADijRAQAAAAAAAABwQIkOAAAAAAAAAIADSnQAAAAAAAAAABwEZHegv7//zTyPLIWFhVnPnD9/vvVMl8tlPTM9Pd16piTly5fPemZycrL1zL+a8uXLW88sUaKE9cwFCxZYz9y0aZP1zG7dulnPlKQPPvjAeuahQ4esZwYEZPuvTp975513rGf6Yp754u+W0NBQ65m///679UzJNz+PTZ8+3Xrms88+az3zevz000/WM3/++WfrmUlJSdYzjx8/bj3TF/8ekHzz9+isWbOsZ7Zv39565rV64IEHrGdu377deuaYMWOsZzZt2tR6ZrNmzaxnStLOnTutZ3bp0sV6pi+uo+v5t9Z33313A88ke+bNm2c9s0qVKtYzffGz41tvvWU9U5Lq169vPTMwMNB65t69e61nXg13ogMAAAAAAAAA4IASHQAAAAAAAAAAB5ToAAAAAAAAAAA4oEQHAAAAAAAAAMABJToAAAAAAAAAAA4o0QEAAAAAAAAAcECJDgAAAAAAAACAA0p0AAAAAAAAAAAcUKIDAAAAAAAAAOCAEh0AAAAAAAAAAAeU6AAAAAAAAAAAOKBEBwAAAAAAAADAASU6AAAAAAAAAAAOKNEBAAAAAAAAAHBAiQ4AAAAAAAAAgANKdAAAAAAAAAAAHFCiAwAAAAAAAADggBIdAAAAAAAAAAAHlOgAAAAAAAAAADigRAcAAAAAAAAAwAElOgAAAAAAAAAADijRAQAAAAAAAABwQIkOAAAAAAAAAIADSnQAAAAAAAAAABxQogMAAAAAAAAA4IASHQAAAAAAAAAAB5ToAAAAAAAAAAA4oEQHAAAAAAAAAMABJToAAAAAAAAAAA4Csj0wINtDb5jjx49bz6xZs6b1TD8/++9lREZGWs+UpNq1a1vPnDJlivXMv5r169dbzyxUqJD1zMDAQOuZFStWtJ753XffWc+UpEqVKlnP9Pf3t55ZtGhR65nXavXq1dYzw8PDrWdu3brVeuaAAQOsZz7xxBPWMyXf/JxStmxZ65m//PKL9cyHHnromh+7YsWKG3gm2bNt2zbrmenp6dYzO3fubD3zyJEj1jMlqXnz5tYzfXHtrly50nrmtZo/f771zOTkZOuZ77//vvXMlJQU65l9+vSxnilJM2bMsJ7Zq1cv65kZGRnWM7t163bNj23ZsuUNPJPsCQsLs555+PBh65m9e/e2nrl3717rmZKUJ08e65m+6PKOHj1qPfNquBMdAAAAAAAAAAAHlOgAAAAAAAAAADigRAcAAAAAAAAAwAElOgAAAAAAAAAADijRAQAAAAAAAABwQIkOAAAAAAAAAIADSnQAAAAAAAAAABxQogMAAAAAAAAA4IASHQAAAAAAAAAAB5ToAAAAAAAAAAA4oEQHAAAAAAAAAMABJToAAAAAAAAAAA4o0QEAAAAAAAAAcECJDgAAAAAAAACAA0p0AAAAAAAAAAAcUKIDAAAAAAAAAOCAEh0AAAAAAAAAAAeU6AAAAAAAAAAAOKBEBwAAAAAAAADAASU6AAAAAAAAAAAOKNEBAAAAAAAAAHBAiQ4AAAAAAAAAgANKdAAAAAAAAAAAHFCiAwAAAAAAAADggBIdAAAAAAAAAAAHlOgAAAAAAAAAADigRAcAAAAAAAAAwAElOgAAAAAAAAAADgKyO9AYczPPI0vbtm2znlmjRg3rme3bt7ee2aZNG+uZkuTnZ/99m7feest6pi/my/X49ddfrWc2aNDAeubDDz9sPbNPnz7WM6tVq2Y9U5Jy585tPXPt2rXWM9PS0qxnXqtjx45Zz2zatKn1TF9c85MnT7aeuX37duuZkrR582brmTVr1rSeOWHCBOuZQ4cOvebHNm7c+MadSDb5+/tbzzx37pz1zGnTplnPLFasmPVMSXrggQesZy5fvtx65oABA6xnXquRI0daz3zllVesZ+7evdt65tGjR61njho1ynqmJPXq1ct6pi/+PJk5c6b1zOuxY8cO65mlSpWynhkREWE98/Dhw9YzIyMjrWdKvvk3ni9+Nv/ggw+sZ14Nd6IDAAAAAAAAAOCAEh0AAAAAAAAAAAeU6AAAAAAAAAAAOKBEBwAAAAAAAADAASU6AAAAAAAAAAAOKNEBAAAAAAAAAHBAiQ4AAAAAAAAAgANKdAAAAAAAAAAAHFCiAwAAAAAAAADggBIdAAAAAAAAAAAHlOgAAAAAAAAAADigRAcAAAAAAAAAwAElOgAAAAAAAAAADijRAQAAAAAAAABwQIkOAAAAAAAAAIADSnQAAAAAAAAAABxQogMAAAAAAAAA4IASHQAAAAAAAAAAB5ToAAAAAAAAAAA4oEQHAAAAAAAAAMABJToAAAAAAAAAAA4o0QEAAAAAAAAAcECJDgAAAAAAAACAA0p0AAAAAAAAAAAcUKIDAAAAAAAAAOCAEh0AAAAAAAAAAAeU6AAAAAAAAAAAOKBEBwAAAAAAAADAQUB2B7pcrpt5Hlnav3+/9cwXXnjBeuarr75qPfPXX3+1nilJ1atXt57p7+9vPTMjI8N65vW46667rGcuXbrUemZgYKD1zGnTplnPHDRokPVMSZoxY4b1zLx581rP3Lp1q/XMa1W0aFHrmT169LCeeeDAAeuZvnhtp0+fbj1Tknbs2GE9MyQkxHpm/fr1rWdej3Xr1lnPzJUrl/XMRYsWWc+sU6eO9cyDBw9az5SkiRMnWs9s0aKF9cx9+/ZZz7xWQ4cOtZ65ceNG65m33Xab9czSpUtbz1y8eLH1TEk6f/689czt27dbzyxWrJj1zOtRoUIF65mbN2+2ntmzZ0/rmd988431zOTkZOuZkrR+/Xrrme+88471zFvx727uRAcAAAAAAAAAwAElOgAAAAAAAAAADijRAQAAAAAAAABwQIkOAAAAAAAAAIADSnQAAAAAAAAAABxQogMAAAAAAAAA4IASHQAAAAAAAAAAB5ToAAAAAAAAAAA4oEQHAAAAAAAAAMABJToAAAAAAAAAAA4o0QEAAAAAAAAAcECJDgAAAAAAAACAA0p0AAAAAAAAAAAcUKIDAAAAAAAAAOCAEh0AAAAAAAAAAAeU6AAAAAAAAAAAOKBEBwAAAAAAAADAASU6AAAAAAAAAAAOKNEBAAAAAAAAAHBAiQ4AAAAAAAAAgANKdAAAAAAAAAAAHFCiAwAAAAAAAADggBIdAAAAAAAAAAAHlOgAAAAAAAAAADigRAcAAAAAAAAAwAElOgAAAAAAAAAADijRAQAAAAAAAABwQIkOAAAAAAAAAIADlzHG+PokAAAAAAAAAAC4FXEnOgAAAAAAAAAADijRAQAAAAAAAABwQIkOAAAAAAAAAIADSnQAAAAAAAAAABxQogMAAAAAAAAA4IASHQAAAAAAAAAAB5ToAAAAAAAAAAA4oEQHAAAAAAAAAMABJToAAAAAAAAAAA7+H8tR99UdHOnlAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 1500x300 with 6 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Show the forward diffusion process\n",
    "sample_image = X_tensor[0:1].to(device)\n",
    "timesteps = [0, 100, 300, 500, 700, 999]\n",
    "\n",
    "fig, axes = plt.subplots(1, len(timesteps), figsize=(15, 3))\n",
    "fig.suptitle('Forward Diffusion Process (Adding Noise)', fontsize=14)\n",
    "\n",
    "for i, t in enumerate(timesteps):\n",
    "    t_tensor = torch.tensor([t], device=device)\n",
    "    noisy_image = scheduler.add_noise(sample_image, t_tensor)\n",
    "    \n",
    "    axes[i].imshow(noisy_image[0, 0].cpu().numpy(), cmap='gray')\n",
    "    axes[i].set_title(f't = {t}')\n",
    "    axes[i].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}